{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üöÄ MOUAADNET-ULTRA Training\n",
                "## Human Detection & Gender Classification with PA-100k\n",
                "\n",
                "**Lead Architect:** MOUAAD IDOUFKIR\n",
                "\n",
                "### üîß Features:\n",
                "- **Dynamic Loss Balancing** using Uncertainty Weighting (Kendall et al.)\n",
                "- **GradNorm** for gradient magnitude normalization\n",
                "- Automatic task weight learning\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!nvidia-smi\n",
                "!git clone https://github.com/mouuuuaad/MouaadNet-Ultra.git\n",
                "%cd MouaadNet-Ultra\n",
                "!pip install -q torch torchvision tqdm scipy kagglehub"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import kagglehub\n",
                "DATA_PATH = kagglehub.dataset_download(\"yuulind/pa-100k\")\n",
                "print(f\"‚úÖ Dataset: {DATA_PATH}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from torchvision import transforms\n",
                "from PIL import Image\n",
                "from scipy.io import loadmat\n",
                "from tqdm import tqdm\n",
                "\n",
                "class PA100kDataset(Dataset):\n",
                "    def __init__(self, root_dir, split='train', img_size=416, transform=None):\n",
                "        self.root_dir = root_dir\n",
                "        self.img_size = img_size\n",
                "        self.transform = transform or self._default_transform()\n",
                "        self.split = split\n",
                "        self._find_data()\n",
                "        self._load_data()\n",
                "        print(f\"‚úÖ {split}: {len(self.images)} images\")\n",
                "    \n",
                "    def _find_data(self):\n",
                "        self.anno_path = None\n",
                "        self.img_dir = self.root_dir\n",
                "        for root, _, files in os.walk(self.root_dir):\n",
                "            for f in files:\n",
                "                if f.endswith('.mat'):\n",
                "                    self.anno_path = os.path.join(root, f)\n",
                "            imgs = [x for x in files if x.lower().endswith(('.jpg', '.png'))]\n",
                "            if len(imgs) > 100:\n",
                "                self.img_dir = root\n",
                "    \n",
                "    def _load_data(self):\n",
                "        if self.anno_path:\n",
                "            anno = loadmat(self.anno_path)\n",
                "            key = f'{self.split}_images_name'\n",
                "            if key in anno:\n",
                "                self.images = [str(x[0][0]) for x in anno[key]]\n",
                "                self.labels = anno[f'{self.split}_label']\n",
                "                return\n",
                "        all_imgs = sorted([f for f in os.listdir(self.img_dir) if f.lower().endswith(('.jpg', '.png'))])\n",
                "        n = len(all_imgs)\n",
                "        splits = {'train': (0, 0.8), 'val': (0.8, 0.9), 'test': (0.9, 1.0)}\n",
                "        s, e = splits[self.split]\n",
                "        self.images = all_imgs[int(s*n):int(e*n)]\n",
                "        self.labels = None\n",
                "    \n",
                "    def _default_transform(self):\n",
                "        return transforms.Compose([\n",
                "            transforms.Resize((self.img_size, self.img_size)),\n",
                "            transforms.ToTensor(),\n",
                "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
                "        ])\n",
                "    \n",
                "    def __len__(self): return len(self.images)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        try:\n",
                "            image = Image.open(os.path.join(self.img_dir, self.images[idx])).convert('RGB')\n",
                "        except:\n",
                "            return self.__getitem__((idx + 1) % len(self))\n",
                "        \n",
                "        if self.transform:\n",
                "            image = self.transform(image)\n",
                "        \n",
                "        # Heatmap\n",
                "        hm_size = self.img_size // 4\n",
                "        cx, cy, sigma = hm_size // 2, hm_size // 2, hm_size // 6\n",
                "        x, y = np.meshgrid(np.arange(hm_size), np.arange(hm_size))\n",
                "        heatmap = np.exp(-((x - cx)**2 + (y - cy)**2) / (2 * sigma**2 + 1e-6)).astype(np.float32)\n",
                "        \n",
                "        # Gender\n",
                "        gender = 1.0 - float(self.labels[idx][0]) if self.labels is not None else 0.5\n",
                "        \n",
                "        return {\n",
                "            'image': image,\n",
                "            'heatmap': torch.from_numpy(heatmap).unsqueeze(0),\n",
                "            'gender': torch.tensor([gender], dtype=torch.float32),\n",
                "        }\n",
                "\n",
                "def create_dataloaders(data_dir, batch_size=32, img_size=416):\n",
                "    train_tf = transforms.Compose([\n",
                "        transforms.Resize((img_size + 32, img_size + 32)),\n",
                "        transforms.RandomCrop(img_size),\n",
                "        transforms.RandomHorizontalFlip(0.5),\n",
                "        transforms.ColorJitter(0.2, 0.2, 0.2),\n",
                "        transforms.ToTensor(),\n",
                "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
                "    ])\n",
                "    val_tf = transforms.Compose([\n",
                "        transforms.Resize((img_size, img_size)),\n",
                "        transforms.ToTensor(),\n",
                "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
                "    ])\n",
                "    train_ds = PA100kDataset(data_dir, 'train', img_size, train_tf)\n",
                "    val_ds = PA100kDataset(data_dir, 'val', img_size, val_tf)\n",
                "    return DataLoader(train_ds, batch_size, shuffle=True, num_workers=2, pin_memory=True), \\\n",
                "           DataLoader(val_ds, batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
                "\n",
                "BATCH_SIZE = 32\n",
                "IMG_SIZE = 416\n",
                "train_loader, val_loader = create_dataloaders(DATA_PATH, BATCH_SIZE, IMG_SIZE)\n",
                "print(f\"Train: {len(train_loader)} batches | Val: {len(val_loader)} batches\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Dynamic Loss Balancing (SOTA Methods)\n",
                "\n",
                "### Method 1: **Uncertainty Weighting** (Kendall et al. 2018)\n",
                "Learns task-specific uncertainty parameters $\\sigma_i$ to automatically balance losses:\n",
                "\n",
                "$$\\mathcal{L}_{total} = \\frac{1}{2\\sigma_1^2}\\mathcal{L}_{heatmap} + \\frac{1}{2\\sigma_2^2}\\mathcal{L}_{gender} + \\log(\\sigma_1\\sigma_2)$$\n",
                "\n",
                "### Method 2: **GradNorm** (Chen et al. 2018)\n",
                "Normalizes gradient magnitudes across tasks to ensure balanced training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.insert(0, '.')\n",
                "from mouaadnet_ultra.model import MouaadNetUltra\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Device: {device}\")\n",
                "\n",
                "model = MouaadNetUltra().to(device)\n",
                "print(f\"Parameters: {model.count_parameters():,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# üî• SOTA: UNCERTAINTY WEIGHTING LOSS\n",
                "# ============================================\n",
                "# Paper: \"Multi-Task Learning Using Uncertainty to Weigh Losses\"\n",
                "# Authors: Kendall, Gal, and Cipolla (CVPR 2018)\n",
                "# \n",
                "# Key Idea: Learn task-specific homoscedastic uncertainty (log_var)\n",
                "# that automatically balances the contribution of each task.\n",
                "# High uncertainty = lower weight, Low uncertainty = higher weight\n",
                "\n",
                "class UncertaintyWeightedLoss(nn.Module):\n",
                "    \"\"\"\n",
                "    Multi-Task Loss with Learnable Uncertainty Weights.\n",
                "    \n",
                "    L_total = (1/2œÉ‚ÇÅ¬≤)¬∑L_hm + (1/2œÉ‚ÇÇ¬≤)¬∑L_gender + log(œÉ‚ÇÅ) + log(œÉ‚ÇÇ)\n",
                "    \n",
                "    We learn log(œÉ¬≤) for numerical stability.\n",
                "    \"\"\"\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        # Learnable log-variance for each task\n",
                "        # Initialize: log(œÉ¬≤) = 0 ‚Üí œÉ = 1 ‚Üí weight = 0.5\n",
                "        self.log_var_hm = nn.Parameter(torch.zeros(1))      # Detection\n",
                "        self.log_var_gender = nn.Parameter(torch.zeros(1))  # Classification\n",
                "    \n",
                "    def focal_loss(self, pred, target, alpha=2.0, beta=4.0):\n",
                "        \"\"\"Stable Gaussian Focal Loss for heatmaps.\"\"\"\n",
                "        pred = torch.clamp(pred, 1e-6, 1 - 1e-6)\n",
                "        \n",
                "        pos_mask = target.eq(1).float()\n",
                "        neg_mask = target.lt(1).float()\n",
                "        \n",
                "        pos_loss = -torch.log(pred) * torch.pow(1 - pred, alpha) * pos_mask\n",
                "        neg_weight = torch.pow(1 - target, beta)\n",
                "        neg_loss = -torch.log(1 - pred) * torch.pow(pred, alpha) * neg_weight * neg_mask\n",
                "        \n",
                "        num_pos = pos_mask.sum().clamp(min=1)\n",
                "        return (pos_loss.sum() + neg_loss.sum()) / num_pos\n",
                "    \n",
                "    def forward(self, pred_hm, target_hm, pred_gender, target_gender):\n",
                "        # Raw task losses\n",
                "        pred_hm_sig = torch.sigmoid(pred_hm)\n",
                "        loss_hm = self.focal_loss(pred_hm_sig, target_hm)\n",
                "        loss_gender = F.binary_cross_entropy_with_logits(pred_gender, target_gender)\n",
                "        \n",
                "        # Uncertainty weighting: L_task / (2*exp(log_var)) + log_var/2\n",
                "        # exp(-log_var) = 1/œÉ¬≤, log_var/2 = log(œÉ)\n",
                "        precision_hm = torch.exp(-self.log_var_hm)\n",
                "        precision_gender = torch.exp(-self.log_var_gender)\n",
                "        \n",
                "        weighted_hm = precision_hm * loss_hm + self.log_var_hm\n",
                "        weighted_gender = precision_gender * loss_gender + self.log_var_gender\n",
                "        \n",
                "        total = weighted_hm + weighted_gender\n",
                "        \n",
                "        # Compute effective weights for logging\n",
                "        with torch.no_grad():\n",
                "            weight_hm = precision_hm.item()\n",
                "            weight_gender = precision_gender.item()\n",
                "        \n",
                "        return {\n",
                "            'total': total,\n",
                "            'loss_hm': loss_hm,\n",
                "            'loss_gender': loss_gender,\n",
                "            'weight_hm': weight_hm,\n",
                "            'weight_gender': weight_gender,\n",
                "            'sigma_hm': torch.exp(self.log_var_hm / 2).item(),\n",
                "            'sigma_gender': torch.exp(self.log_var_gender / 2).item(),\n",
                "        }\n",
                "\n",
                "print(\"‚úÖ UncertaintyWeightedLoss defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# üî• SOTA: GRADNORM\n",
                "# ============================================\n",
                "# Paper: \"GradNorm: Gradient Normalization for Adaptive Loss Balancing\"\n",
                "# Authors: Chen et al. (ICML 2018)\n",
                "#\n",
                "# Key Idea: Balance gradient magnitudes across tasks by dynamically\n",
                "# adjusting loss weights based on training rate of each task.\n",
                "\n",
                "class GradNormLoss(nn.Module):\n",
                "    \"\"\"\n",
                "    GradNorm: Gradient Normalization for Multi-Task Learning.\n",
                "    \n",
                "    Balances gradient magnitudes to ensure all tasks train at similar rates.\n",
                "    \n",
                "    Args:\n",
                "        num_tasks: Number of tasks\n",
                "        alpha: Asymmetry hyperparameter (higher = more aggressive balancing)\n",
                "    \"\"\"\n",
                "    def __init__(self, num_tasks=2, alpha=1.5):\n",
                "        super().__init__()\n",
                "        self.num_tasks = num_tasks\n",
                "        self.alpha = alpha\n",
                "        \n",
                "        # Learnable task weights (log scale for stability)\n",
                "        self.log_weights = nn.Parameter(torch.zeros(num_tasks))\n",
                "        \n",
                "        # Track initial losses for relative training rate\n",
                "        self.register_buffer('initial_losses', torch.ones(num_tasks))\n",
                "        self.initialized = False\n",
                "    \n",
                "    def focal_loss(self, pred, target):\n",
                "        pred = torch.clamp(torch.sigmoid(pred), 1e-6, 1 - 1e-6)\n",
                "        pos_mask = target.eq(1).float()\n",
                "        neg_mask = target.lt(1).float()\n",
                "        pos_loss = -torch.log(pred) * torch.pow(1 - pred, 2) * pos_mask\n",
                "        neg_loss = -torch.log(1 - pred) * torch.pow(pred, 2) * torch.pow(1 - target, 4) * neg_mask\n",
                "        return (pos_loss.sum() + neg_loss.sum()) / pos_mask.sum().clamp(min=1)\n",
                "    \n",
                "    def forward(self, pred_hm, target_hm, pred_gender, target_gender):\n",
                "        # Compute raw losses\n",
                "        loss_hm = self.focal_loss(pred_hm, target_hm)\n",
                "        loss_gender = F.binary_cross_entropy_with_logits(pred_gender, target_gender)\n",
                "        \n",
                "        losses = torch.stack([loss_hm, loss_gender])\n",
                "        \n",
                "        # Initialize on first forward\n",
                "        if not self.initialized:\n",
                "            self.initial_losses = losses.detach().clone()\n",
                "            self.initialized = True\n",
                "        \n",
                "        # Get weights (softmax to ensure sum = num_tasks)\n",
                "        weights = F.softmax(self.log_weights, dim=0) * self.num_tasks\n",
                "        \n",
                "        # Weighted sum\n",
                "        total = (weights * losses).sum()\n",
                "        \n",
                "        return {\n",
                "            'total': total,\n",
                "            'loss_hm': loss_hm,\n",
                "            'loss_gender': loss_gender,\n",
                "            'weight_hm': weights[0].item(),\n",
                "            'weight_gender': weights[1].item(),\n",
                "            'losses': losses,  # For GradNorm update\n",
                "            'weights': weights,  # For GradNorm update\n",
                "        }\n",
                "    \n",
                "    def gradnorm_update(self, losses, shared_layer, lr=0.025):\n",
                "        \"\"\"\n",
                "        Update task weights based on gradient magnitudes.\n",
                "        Call this after backward() on each task separately.\n",
                "        \"\"\"\n",
                "        weights = F.softmax(self.log_weights, dim=0) * self.num_tasks\n",
                "        \n",
                "        # Compute gradient norms for each task\n",
                "        grad_norms = []\n",
                "        for i, loss in enumerate(losses):\n",
                "            # Get gradient w.r.t. shared layer\n",
                "            grads = torch.autograd.grad(loss * weights[i], shared_layer.parameters(), \n",
                "                                        retain_graph=True, allow_unused=True)\n",
                "            grad_norm = sum(g.norm() for g in grads if g is not None)\n",
                "            grad_norms.append(grad_norm)\n",
                "        \n",
                "        grad_norms = torch.stack(grad_norms)\n",
                "        \n",
                "        # Target: average gradient norm\n",
                "        avg_grad = grad_norms.mean()\n",
                "        \n",
                "        # Relative inverse training rate\n",
                "        loss_ratios = losses.detach() / (self.initial_losses + 1e-8)\n",
                "        inverse_train_rate = loss_ratios / loss_ratios.mean()\n",
                "        \n",
                "        # Target gradient for each task\n",
                "        target_grads = avg_grad * (inverse_train_rate ** self.alpha)\n",
                "        \n",
                "        # GradNorm loss: difference between current and target gradient norms\n",
                "        gradnorm_loss = (grad_norms - target_grads).abs().sum()\n",
                "        \n",
                "        # Update weights\n",
                "        gradnorm_loss.backward()\n",
                "        with torch.no_grad():\n",
                "            self.log_weights -= lr * self.log_weights.grad\n",
                "            self.log_weights.grad.zero_()\n",
                "\n",
                "print(\"‚úÖ GradNormLoss defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# SIMPLE BUT EFFECTIVE: LOSS NORMALIZATION\n",
                "# ============================================\n",
                "# Normalize each loss by its running average for implicit balancing\n",
                "\n",
                "class NormalizedMultiTaskLoss(nn.Module):\n",
                "    \"\"\"\n",
                "    Simple Loss Normalization: Divide each loss by its EMA.\n",
                "    This implicitly balances losses without learnable parameters.\n",
                "    \"\"\"\n",
                "    def __init__(self, ema_decay=0.99):\n",
                "        super().__init__()\n",
                "        self.ema_decay = ema_decay\n",
                "        self.register_buffer('ema_hm', torch.ones(1))\n",
                "        self.register_buffer('ema_gender', torch.ones(1))\n",
                "    \n",
                "    def focal_loss(self, pred, target):\n",
                "        pred = torch.clamp(torch.sigmoid(pred), 1e-6, 1 - 1e-6)\n",
                "        pos_mask = target.eq(1).float()\n",
                "        neg_mask = target.lt(1).float()\n",
                "        pos_loss = -torch.log(pred) * torch.pow(1 - pred, 2) * pos_mask\n",
                "        neg_loss = -torch.log(1 - pred) * torch.pow(pred, 2) * torch.pow(1 - target, 4) * neg_mask\n",
                "        return (pos_loss.sum() + neg_loss.sum()) / pos_mask.sum().clamp(min=1)\n",
                "    \n",
                "    def forward(self, pred_hm, target_hm, pred_gender, target_gender):\n",
                "        loss_hm = self.focal_loss(pred_hm, target_hm)\n",
                "        loss_gender = F.binary_cross_entropy_with_logits(pred_gender, target_gender)\n",
                "        \n",
                "        # Update EMA\n",
                "        with torch.no_grad():\n",
                "            self.ema_hm = self.ema_decay * self.ema_hm + (1 - self.ema_decay) * loss_hm\n",
                "            self.ema_gender = self.ema_decay * self.ema_gender + (1 - self.ema_decay) * loss_gender\n",
                "        \n",
                "        # Normalize by EMA (losses become ~1.0 scale)\n",
                "        norm_hm = loss_hm / (self.ema_hm + 1e-8)\n",
                "        norm_gender = loss_gender / (self.ema_gender + 1e-8)\n",
                "        \n",
                "        total = norm_hm + norm_gender\n",
                "        \n",
                "        return {\n",
                "            'total': total,\n",
                "            'loss_hm': loss_hm,\n",
                "            'loss_gender': loss_gender,\n",
                "            'norm_hm': norm_hm.item(),\n",
                "            'norm_gender': norm_gender.item(),\n",
                "        }\n",
                "\n",
                "print(\"‚úÖ NormalizedMultiTaskLoss defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Choose loss function:\n",
                "# Option 1: UncertaintyWeightedLoss (recommended - learns optimal weights)\n",
                "# Option 2: NormalizedMultiTaskLoss (simpler - normalizes by running average)\n",
                "\n",
                "LOSS_TYPE = 'uncertainty'  # 'uncertainty' or 'normalized'\n",
                "\n",
                "if LOSS_TYPE == 'uncertainty':\n",
                "    criterion = UncertaintyWeightedLoss().to(device)\n",
                "    print(\"üìä Using: Uncertainty Weighting (Kendall et al. 2018)\")\n",
                "else:\n",
                "    criterion = NormalizedMultiTaskLoss().to(device)\n",
                "    print(\"üìä Using: EMA-Normalized Loss\")\n",
                "\n",
                "# Training config\n",
                "EPOCHS = 30\n",
                "LR = 5e-4\n",
                "\n",
                "# Include loss parameters in optimizer for uncertainty weighting\n",
                "if LOSS_TYPE == 'uncertainty':\n",
                "    optimizer = torch.optim.AdamW([\n",
                "        {'params': model.parameters(), 'lr': LR},\n",
                "        {'params': criterion.parameters(), 'lr': LR * 10},  # Faster learning for weights\n",
                "    ], weight_decay=1e-4)\n",
                "else:\n",
                "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
                "\n",
                "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-6)\n",
                "scaler = torch.amp.GradScaler('cuda')\n",
                "\n",
                "print(f\"‚úÖ Training: {EPOCHS} epochs, LR={LR}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4Ô∏è‚É£ Training Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_epoch(model, loader, optimizer, criterion, scaler, device):\n",
                "    model.train()\n",
                "    total_loss, total_hm, total_gender = 0, 0, 0\n",
                "    weight_hm_sum, weight_gender_sum = 0, 0\n",
                "    \n",
                "    pbar = tqdm(loader, desc='Training')\n",
                "    for batch in pbar:\n",
                "        images = batch['image'].to(device)\n",
                "        heatmaps = batch['heatmap'].to(device)\n",
                "        genders = batch['gender'].to(device)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        \n",
                "        with torch.amp.autocast('cuda'):\n",
                "            outputs = model(images)\n",
                "            pred_hm = outputs['heatmaps'][0]\n",
                "            pred_gender = outputs['gender']\n",
                "            losses = criterion(pred_hm, heatmaps, pred_gender, genders)\n",
                "            loss = losses['total']\n",
                "        \n",
                "        if torch.isnan(loss) or torch.isinf(loss):\n",
                "            continue\n",
                "        \n",
                "        scaler.scale(loss).backward()\n",
                "        scaler.unscale_(optimizer)\n",
                "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
                "        scaler.step(optimizer)\n",
                "        scaler.update()\n",
                "        \n",
                "        total_loss += loss.item()\n",
                "        total_hm += losses['loss_hm'].item()\n",
                "        total_gender += losses['loss_gender'].item()\n",
                "        \n",
                "        # Track weights if using uncertainty\n",
                "        if 'weight_hm' in losses:\n",
                "            weight_hm_sum += losses['weight_hm']\n",
                "            weight_gender_sum += losses['weight_gender']\n",
                "        \n",
                "        pbar.set_postfix({\n",
                "            'loss': f\"{loss.item():.3f}\",\n",
                "            'hm': f\"{losses['loss_hm'].item():.1f}\",\n",
                "            'gender': f\"{losses['loss_gender'].item():.3f}\",\n",
                "        })\n",
                "    \n",
                "    n = len(loader)\n",
                "    return {\n",
                "        'loss': total_loss/n,\n",
                "        'hm': total_hm/n,\n",
                "        'gender': total_gender/n,\n",
                "        'w_hm': weight_hm_sum/n if weight_hm_sum > 0 else 1.0,\n",
                "        'w_gender': weight_gender_sum/n if weight_gender_sum > 0 else 1.0,\n",
                "    }\n",
                "\n",
                "@torch.no_grad()\n",
                "def validate(model, loader, criterion, device):\n",
                "    model.eval()\n",
                "    total_loss = 0\n",
                "    correct, total = 0, 0\n",
                "    \n",
                "    for batch in tqdm(loader, desc='Validating'):\n",
                "        images = batch['image'].to(device)\n",
                "        heatmaps = batch['heatmap'].to(device)\n",
                "        genders = batch['gender'].to(device)\n",
                "        \n",
                "        with torch.amp.autocast('cuda'):\n",
                "            outputs = model(images)\n",
                "            losses = criterion(outputs['heatmaps'][0], heatmaps, outputs['gender'], genders)\n",
                "        \n",
                "        total_loss += losses['total'].item()\n",
                "        pred = (torch.sigmoid(outputs['gender']) > 0.5).float()\n",
                "        correct += (pred == genders).sum().item()\n",
                "        total += genders.size(0)\n",
                "    \n",
                "    return total_loss / len(loader), correct / total * 100\n",
                "\n",
                "print(\"‚úÖ Training functions ready\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# üöÄ TRAIN!\n",
                "best_acc = 0\n",
                "history = {'loss': [], 'val_loss': [], 'acc': [], 'hm': [], 'gender': [], 'w_hm': [], 'w_gender': []}\n",
                "\n",
                "print(\"=\"*70)\n",
                "print(\"üöÄ Training MOUAADNET-ULTRA with Dynamic Loss Balancing\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "for epoch in range(EPOCHS):\n",
                "    print(f\"\\nüìç Epoch {epoch+1}/{EPOCHS}\")\n",
                "    \n",
                "    train_stats = train_epoch(model, train_loader, optimizer, criterion, scaler, device)\n",
                "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
                "    scheduler.step()\n",
                "    \n",
                "    # Log\n",
                "    history['loss'].append(train_stats['loss'])\n",
                "    history['val_loss'].append(val_loss)\n",
                "    history['acc'].append(val_acc)\n",
                "    history['hm'].append(train_stats['hm'])\n",
                "    history['gender'].append(train_stats['gender'])\n",
                "    history['w_hm'].append(train_stats['w_hm'])\n",
                "    history['w_gender'].append(train_stats['w_gender'])\n",
                "    \n",
                "    print(f\"   üìâ Loss: {train_stats['loss']:.4f} | HM: {train_stats['hm']:.1f} | Gender: {train_stats['gender']:.4f}\")\n",
                "    print(f\"   ‚öñÔ∏è  Weights: HM={train_stats['w_hm']:.4f}, Gender={train_stats['w_gender']:.4f}\")\n",
                "    print(f\"   üìä Val Loss: {val_loss:.4f} | Gender Acc: {val_acc:.2f}%\")\n",
                "    \n",
                "    if val_acc > best_acc:\n",
                "        best_acc = val_acc\n",
                "        torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'best_acc': best_acc}, 'best_model.pt')\n",
                "        print(\"   ‚≠ê Best model saved!\")\n",
                "\n",
                "print(f\"\\n‚úÖ Training complete! Best accuracy: {best_acc:.2f}%\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot results with weight evolution\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
                "\n",
                "# Loss curves\n",
                "axes[0, 0].plot(history['loss'], label='Train')\n",
                "axes[0, 0].plot(history['val_loss'], label='Val')\n",
                "axes[0, 0].set_xlabel('Epoch')\n",
                "axes[0, 0].set_ylabel('Total Loss')\n",
                "axes[0, 0].legend()\n",
                "axes[0, 0].set_title('Total Loss')\n",
                "axes[0, 0].grid(True)\n",
                "\n",
                "# Individual losses\n",
                "axes[0, 1].plot(history['hm'], label='Heatmap', color='blue')\n",
                "ax2 = axes[0, 1].twinx()\n",
                "ax2.plot(history['gender'], label='Gender', color='orange')\n",
                "axes[0, 1].set_xlabel('Epoch')\n",
                "axes[0, 1].set_ylabel('Heatmap Loss', color='blue')\n",
                "ax2.set_ylabel('Gender Loss', color='orange')\n",
                "axes[0, 1].set_title('Task Losses (Different Scales)')\n",
                "\n",
                "# Learned weights\n",
                "axes[1, 0].plot(history['w_hm'], label='Heatmap Weight', color='blue')\n",
                "axes[1, 0].plot(history['w_gender'], label='Gender Weight', color='orange')\n",
                "axes[1, 0].set_xlabel('Epoch')\n",
                "axes[1, 0].set_ylabel('Weight')\n",
                "axes[1, 0].legend()\n",
                "axes[1, 0].set_title('Learned Task Weights (Dynamic Balancing)')\n",
                "axes[1, 0].grid(True)\n",
                "\n",
                "# Accuracy\n",
                "axes[1, 1].plot(history['acc'], color='green', linewidth=2)\n",
                "axes[1, 1].set_xlabel('Epoch')\n",
                "axes[1, 1].set_ylabel('Accuracy (%)')\n",
                "axes[1, 1].set_title(f'Gender Classification Accuracy (Best: {best_acc:.2f}%)')\n",
                "axes[1, 1].grid(True)\n",
                "\n",
                "plt.suptitle('MOUAADNET-ULTRA Training with Uncertainty Weighting', fontsize=14)\n",
                "plt.tight_layout()\n",
                "plt.savefig('training_dynamic.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5Ô∏è‚É£ Export"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ckpt = torch.load('best_model.pt')\n",
                "model.load_state_dict(ckpt['model_state_dict'])\n",
                "model.eval()\n",
                "model.fuse_for_inference()\n",
                "model.cpu()\n",
                "\n",
                "torch.onnx.export(model, torch.randn(1, 3, 416, 416), 'mouaadnet_ultra.onnx', input_names=['image'], opset_version=12)\n",
                "print(f\"‚úÖ Exported! Best accuracy: {ckpt['best_acc']:.2f}%\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import files\n",
                "files.download('best_model.pt')\n",
                "files.download('mouaadnet_ultra.onnx')\n",
                "files.download('training_dynamic.png')\n",
                "print(\"üéâ Done!\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}