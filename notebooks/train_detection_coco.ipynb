{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üéØ MOUAADNET-ULTRA: Human Detection Training\n",
                "## Using COCO Person Dataset (Real Bounding Boxes)\n",
                "\n",
                "**Lead Architect:** MOUAAD IDOUFKIR\n",
                "\n",
                "### ‚ö†Ô∏è Why PA-100k Failed for Detection:\n",
                "PA-100k contains **cropped pedestrian images** without scene context.\n",
                "For proper detection, we need **full scenes with bounding boxes**.\n",
                "\n",
                "### ‚úÖ This Notebook Uses:\n",
                "- **COCO 2017** - Real bounding box annotations\n",
                "- **CenterNet-style** heatmap generation\n",
                "- Proper detection training\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!nvidia-smi\n",
                "!git clone https://github.com/mouuuuaad/MouaadNet-Ultra.git\n",
                "%cd MouaadNet-Ultra\n",
                "!pip install -q torch torchvision tqdm pycocotools"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Download COCO 2017 (Person Only)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Download COCO 2017 Train images\n",
                "!mkdir -p data/coco\n",
                "!wget -q http://images.cocodataset.org/zips/train2017.zip -O data/coco/train2017.zip\n",
                "!wget -q http://images.cocodataset.org/zips/val2017.zip -O data/coco/val2017.zip\n",
                "!wget -q http://images.cocodataset.org/annotations/annotations_trainval2017.zip -O data/coco/annotations.zip\n",
                "\n",
                "!cd data/coco && unzip -q train2017.zip && unzip -q val2017.zip && unzip -q annotations.zip\n",
                "!rm data/coco/*.zip\n",
                "print(\"‚úÖ COCO 2017 downloaded!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Alternative: Use smaller subset for faster training\n",
                "# Skip this cell if you downloaded full COCO above\n",
                "\n",
                "# !pip install -q fiftyone\n",
                "# import fiftyone.zoo as foz\n",
                "# dataset = foz.load_zoo_dataset(\n",
                "#     \"coco-2017\",\n",
                "#     split=\"train\",\n",
                "#     label_types=[\"detections\"],\n",
                "#     classes=[\"person\"],\n",
                "#     max_samples=10000,  # Smaller subset\n",
                "# )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ COCO Person Dataset with Heatmap Generation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from torchvision import transforms\n",
                "from PIL import Image\n",
                "from pycocotools.coco import COCO\n",
                "from tqdm import tqdm\n",
                "import cv2\n",
                "\n",
                "\n",
                "def gaussian2D(shape, sigma=1):\n",
                "    \"\"\"Generate 2D Gaussian kernel.\"\"\"\n",
                "    m, n = [(ss - 1.) / 2. for ss in shape]\n",
                "    y, x = np.ogrid[-m:m+1, -n:n+1]\n",
                "    h = np.exp(-(x*x + y*y) / (2*sigma*sigma))\n",
                "    h[h < np.finfo(h.dtype).eps * h.max()] = 0\n",
                "    return h\n",
                "\n",
                "\n",
                "def draw_gaussian(heatmap, center, radius, k=1):\n",
                "    \"\"\"Draw Gaussian on heatmap at center with given radius.\"\"\"\n",
                "    diameter = 2 * radius + 1\n",
                "    gaussian = gaussian2D((diameter, diameter), sigma=diameter / 6)\n",
                "    \n",
                "    x, y = int(center[0]), int(center[1])\n",
                "    height, width = heatmap.shape[0:2]\n",
                "    \n",
                "    left, right = min(x, radius), min(width - x, radius + 1)\n",
                "    top, bottom = min(y, radius), min(height - y, radius + 1)\n",
                "    \n",
                "    masked_heatmap = heatmap[y - top:y + bottom, x - left:x + right]\n",
                "    masked_gaussian = gaussian[radius - top:radius + bottom, radius - left:radius + right]\n",
                "    \n",
                "    if min(masked_gaussian.shape) > 0 and min(masked_heatmap.shape) > 0:\n",
                "        np.maximum(masked_heatmap, masked_gaussian * k, out=masked_heatmap)\n",
                "    \n",
                "    return heatmap\n",
                "\n",
                "\n",
                "def gaussian_radius(det_size, min_overlap=0.7):\n",
                "    \"\"\"Compute Gaussian radius based on object size.\"\"\"\n",
                "    height, width = det_size\n",
                "    a1 = 1\n",
                "    b1 = (height + width)\n",
                "    c1 = width * height * (1 - min_overlap) / (1 + min_overlap)\n",
                "    sq1 = np.sqrt(b1 ** 2 - 4 * a1 * c1)\n",
                "    r1 = (b1 + sq1) / 2\n",
                "    return max(0, int(r1))\n",
                "\n",
                "\n",
                "class COCOPersonDataset(Dataset):\n",
                "    \"\"\"\n",
                "    COCO Dataset for Person Detection.\n",
                "    Generates CenterNet-style heatmaps with proper bounding boxes.\n",
                "    \"\"\"\n",
                "    PERSON_CAT_ID = 1  # Person category ID in COCO\n",
                "    \n",
                "    def __init__(self, root_dir, split='train', img_size=416, down_ratio=4):\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            root_dir: COCO root directory\n",
                "            split: 'train' or 'val'\n",
                "            img_size: Input image size\n",
                "            down_ratio: Heatmap downsampling ratio\n",
                "        \"\"\"\n",
                "        self.root = root_dir\n",
                "        self.split = split\n",
                "        self.img_size = img_size\n",
                "        self.down_ratio = down_ratio\n",
                "        self.output_size = img_size // down_ratio\n",
                "        \n",
                "        # Load COCO annotations\n",
                "        anno_file = os.path.join(root_dir, 'annotations', f'instances_{split}2017.json')\n",
                "        self.coco = COCO(anno_file)\n",
                "        \n",
                "        # Get all images containing persons\n",
                "        self.img_ids = self.coco.getImgIds(catIds=[self.PERSON_CAT_ID])\n",
                "        print(f\"‚úÖ Loaded {len(self.img_ids)} images with persons ({split})\")\n",
                "        \n",
                "        # Transforms\n",
                "        self.transform = transforms.Compose([\n",
                "            transforms.ToTensor(),\n",
                "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
                "        ])\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.img_ids)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        img_id = self.img_ids[idx]\n",
                "        img_info = self.coco.loadImgs(img_id)[0]\n",
                "        img_path = os.path.join(self.root, f'{self.split}2017', img_info['file_name'])\n",
                "        \n",
                "        # Load image\n",
                "        img = cv2.imread(img_path)\n",
                "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
                "        orig_h, orig_w = img.shape[:2]\n",
                "        \n",
                "        # Resize with aspect ratio\n",
                "        scale = min(self.img_size / orig_h, self.img_size / orig_w)\n",
                "        new_h, new_w = int(orig_h * scale), int(orig_w * scale)\n",
                "        img_resized = cv2.resize(img, (new_w, new_h))\n",
                "        \n",
                "        # Pad to square\n",
                "        pad_h = self.img_size - new_h\n",
                "        pad_w = self.img_size - new_w\n",
                "        pad_top = pad_h // 2\n",
                "        pad_left = pad_w // 2\n",
                "        \n",
                "        img_padded = np.full((self.img_size, self.img_size, 3), 114, dtype=np.uint8)\n",
                "        img_padded[pad_top:pad_top+new_h, pad_left:pad_left+new_w] = img_resized\n",
                "        \n",
                "        # Get annotations\n",
                "        ann_ids = self.coco.getAnnIds(imgIds=img_id, catIds=[self.PERSON_CAT_ID], iscrowd=False)\n",
                "        anns = self.coco.loadAnns(ann_ids)\n",
                "        \n",
                "        # Generate targets\n",
                "        heatmap = np.zeros((self.output_size, self.output_size), dtype=np.float32)\n",
                "        size_map = np.zeros((2, self.output_size, self.output_size), dtype=np.float32)\n",
                "        offset_map = np.zeros((2, self.output_size, self.output_size), dtype=np.float32)\n",
                "        reg_mask = np.zeros((self.output_size, self.output_size), dtype=np.float32)\n",
                "        \n",
                "        num_persons = 0\n",
                "        for ann in anns:\n",
                "            bbox = ann['bbox']  # [x, y, w, h]\n",
                "            if bbox[2] < 5 or bbox[3] < 5:  # Skip tiny boxes\n",
                "                continue\n",
                "            \n",
                "            # Scale bbox to resized image\n",
                "            x = bbox[0] * scale + pad_left\n",
                "            y = bbox[1] * scale + pad_top\n",
                "            w = bbox[2] * scale\n",
                "            h = bbox[3] * scale\n",
                "            \n",
                "            # Center in output space\n",
                "            cx = (x + w / 2) / self.down_ratio\n",
                "            cy = (y + h / 2) / self.down_ratio\n",
                "            \n",
                "            # Clip to output size\n",
                "            cx = np.clip(cx, 0, self.output_size - 1)\n",
                "            cy = np.clip(cy, 0, self.output_size - 1)\n",
                "            \n",
                "            # Integer center\n",
                "            cx_int, cy_int = int(cx), int(cy)\n",
                "            \n",
                "            # Gaussian radius based on object size\n",
                "            radius = gaussian_radius((h / self.down_ratio, w / self.down_ratio))\n",
                "            radius = max(1, radius)\n",
                "            \n",
                "            # Draw Gaussian on heatmap\n",
                "            draw_gaussian(heatmap, (cx_int, cy_int), radius)\n",
                "            \n",
                "            # Size and offset (normalized)\n",
                "            size_map[0, cy_int, cx_int] = w / self.img_size\n",
                "            size_map[1, cy_int, cx_int] = h / self.img_size\n",
                "            offset_map[0, cy_int, cx_int] = cx - cx_int\n",
                "            offset_map[1, cy_int, cx_int] = cy - cy_int\n",
                "            reg_mask[cy_int, cx_int] = 1\n",
                "            \n",
                "            num_persons += 1\n",
                "        \n",
                "        # Apply transforms\n",
                "        img_tensor = self.transform(Image.fromarray(img_padded))\n",
                "        \n",
                "        return {\n",
                "            'image': img_tensor,\n",
                "            'heatmap': torch.from_numpy(heatmap).unsqueeze(0),\n",
                "            'size': torch.from_numpy(size_map),\n",
                "            'offset': torch.from_numpy(offset_map),\n",
                "            'reg_mask': torch.from_numpy(reg_mask),\n",
                "            'num_persons': num_persons,\n",
                "        }\n",
                "\n",
                "\n",
                "print(\"‚úÖ Dataset class ready\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create dataloaders\n",
                "BATCH_SIZE = 16  # Smaller due to larger images\n",
                "IMG_SIZE = 416\n",
                "DATA_DIR = 'data/coco'\n",
                "\n",
                "train_dataset = COCOPersonDataset(DATA_DIR, 'train', IMG_SIZE)\n",
                "val_dataset = COCOPersonDataset(DATA_DIR, 'val', IMG_SIZE)\n",
                "\n",
                "train_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
                "val_loader = DataLoader(val_dataset, BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
                "\n",
                "print(f\"\\nüìä Data: {len(train_loader)} train batches, {len(val_loader)} val batches\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize sample\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "batch = next(iter(train_loader))\n",
                "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
                "\n",
                "for i in range(4):\n",
                "    # Image\n",
                "    img = batch['image'][i].permute(1, 2, 0).numpy()\n",
                "    img = img * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]\n",
                "    img = np.clip(img, 0, 1)\n",
                "    axes[0, i].imshow(img)\n",
                "    axes[0, i].set_title(f\"Persons: {batch['num_persons'][i]}\")\n",
                "    axes[0, i].axis('off')\n",
                "    \n",
                "    # Heatmap\n",
                "    hm = batch['heatmap'][i, 0].numpy()\n",
                "    axes[1, i].imshow(hm, cmap='hot')\n",
                "    axes[1, i].set_title('Detection Heatmap')\n",
                "    axes[1, i].axis('off')\n",
                "\n",
                "plt.suptitle('COCO Person Dataset - Real Bounding Boxes!', fontsize=14)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4Ô∏è‚É£ Detection-Focused Loss"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.insert(0, '.')\n",
                "from mouaadnet_ultra.model import MouaadNetUltra\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "model = MouaadNetUltra().to(device)\n",
                "print(f\"Device: {device}\")\n",
                "print(f\"Parameters: {model.count_parameters():,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class DetectionLoss(nn.Module):\n",
                "    \"\"\"\n",
                "    CenterNet-style detection loss.\n",
                "    \n",
                "    Components:\n",
                "    - Focal Loss for heatmap\n",
                "    - L1 Loss for size regression\n",
                "    - L1 Loss for offset regression\n",
                "    \"\"\"\n",
                "    def __init__(self, hm_weight=1.0, size_weight=0.1, offset_weight=1.0):\n",
                "        super().__init__()\n",
                "        self.hm_weight = hm_weight\n",
                "        self.size_weight = size_weight\n",
                "        self.offset_weight = offset_weight\n",
                "    \n",
                "    def focal_loss(self, pred, target):\n",
                "        \"\"\"Focal loss for heatmap.\"\"\"\n",
                "        pred = torch.clamp(torch.sigmoid(pred), 1e-6, 1 - 1e-6)\n",
                "        \n",
                "        pos_mask = target.eq(1).float()\n",
                "        neg_mask = target.lt(1).float()\n",
                "        \n",
                "        pos_loss = -torch.log(pred) * torch.pow(1 - pred, 2) * pos_mask\n",
                "        neg_loss = -torch.log(1 - pred) * torch.pow(pred, 2) * torch.pow(1 - target, 4) * neg_mask\n",
                "        \n",
                "        num_pos = pos_mask.sum().clamp(min=1)\n",
                "        return (pos_loss.sum() + neg_loss.sum()) / num_pos\n",
                "    \n",
                "    def reg_loss(self, pred, target, mask):\n",
                "        \"\"\"L1 loss for regression with mask.\"\"\"\n",
                "        mask = mask.unsqueeze(1).expand_as(pred)\n",
                "        loss = F.l1_loss(pred * mask, target * mask, reduction='sum')\n",
                "        num_pos = mask.sum().clamp(min=1)\n",
                "        return loss / num_pos\n",
                "    \n",
                "    def forward(self, pred_hm, pred_size, pred_offset, target_hm, target_size, target_offset, reg_mask):\n",
                "        hm_loss = self.focal_loss(pred_hm, target_hm)\n",
                "        size_loss = self.reg_loss(pred_size, target_size, reg_mask)\n",
                "        offset_loss = self.reg_loss(pred_offset, target_offset, reg_mask)\n",
                "        \n",
                "        total = self.hm_weight * hm_loss + self.size_weight * size_loss + self.offset_weight * offset_loss\n",
                "        \n",
                "        return {\n",
                "            'total': total,\n",
                "            'hm': hm_loss,\n",
                "            'size': size_loss,\n",
                "            'offset': offset_loss,\n",
                "        }\n",
                "\n",
                "criterion = DetectionLoss(hm_weight=1.0, size_weight=0.1, offset_weight=1.0)\n",
                "print(\"‚úÖ Detection loss ready\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "EPOCHS = 30\n",
                "LR = 1e-3\n",
                "\n",
                "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
                "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-6)\n",
                "scaler = torch.amp.GradScaler('cuda')\n",
                "\n",
                "print(f\"‚úÖ Training setup: {EPOCHS} epochs, LR={LR}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5Ô∏è‚É£ Training Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_epoch(model, loader, optimizer, criterion, scaler, device):\n",
                "    model.train()\n",
                "    total_loss, total_hm, total_size, total_offset = 0, 0, 0, 0\n",
                "    \n",
                "    pbar = tqdm(loader, desc='Training')\n",
                "    for batch in pbar:\n",
                "        images = batch['image'].to(device)\n",
                "        heatmaps = batch['heatmap'].to(device)\n",
                "        sizes = batch['size'].to(device)\n",
                "        offsets = batch['offset'].to(device)\n",
                "        reg_mask = batch['reg_mask'].to(device)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        \n",
                "        with torch.amp.autocast('cuda'):\n",
                "            outputs = model(images)\n",
                "            \n",
                "            # Use first scale (104x104)\n",
                "            pred_hm = outputs['heatmaps'][0]\n",
                "            pred_size = outputs['sizes'][0]\n",
                "            pred_offset = outputs['offsets'][0]\n",
                "            \n",
                "            losses = criterion(pred_hm, pred_size, pred_offset, heatmaps, sizes, offsets, reg_mask)\n",
                "            loss = losses['total']\n",
                "        \n",
                "        if torch.isnan(loss):\n",
                "            continue\n",
                "        \n",
                "        scaler.scale(loss).backward()\n",
                "        scaler.unscale_(optimizer)\n",
                "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
                "        scaler.step(optimizer)\n",
                "        scaler.update()\n",
                "        \n",
                "        total_loss += loss.item()\n",
                "        total_hm += losses['hm'].item()\n",
                "        total_size += losses['size'].item()\n",
                "        total_offset += losses['offset'].item()\n",
                "        \n",
                "        pbar.set_postfix({'loss': f\"{loss.item():.4f}\", 'hm': f\"{losses['hm'].item():.3f}\"})\n",
                "    \n",
                "    n = len(loader)\n",
                "    return {'loss': total_loss/n, 'hm': total_hm/n, 'size': total_size/n, 'offset': total_offset/n}\n",
                "\n",
                "\n",
                "@torch.no_grad()\n",
                "def validate(model, loader, criterion, device):\n",
                "    model.eval()\n",
                "    total_loss = 0\n",
                "    \n",
                "    for batch in tqdm(loader, desc='Validating'):\n",
                "        images = batch['image'].to(device)\n",
                "        heatmaps = batch['heatmap'].to(device)\n",
                "        sizes = batch['size'].to(device)\n",
                "        offsets = batch['offset'].to(device)\n",
                "        reg_mask = batch['reg_mask'].to(device)\n",
                "        \n",
                "        with torch.amp.autocast('cuda'):\n",
                "            outputs = model(images)\n",
                "            losses = criterion(outputs['heatmaps'][0], outputs['sizes'][0], outputs['offsets'][0],\n",
                "                             heatmaps, sizes, offsets, reg_mask)\n",
                "        \n",
                "        total_loss += losses['total'].item()\n",
                "    \n",
                "    return total_loss / len(loader)\n",
                "\n",
                "print(\"‚úÖ Training functions ready\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# üöÄ TRAIN DETECTION MODEL!\n",
                "best_loss = float('inf')\n",
                "history = {'loss': [], 'val_loss': [], 'hm': [], 'size': [], 'offset': []}\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"üéØ Training MOUAADNET-ULTRA for HUMAN DETECTION\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "for epoch in range(EPOCHS):\n",
                "    print(f\"\\nüìç Epoch {epoch+1}/{EPOCHS}\")\n",
                "    \n",
                "    stats = train_epoch(model, train_loader, optimizer, criterion, scaler, device)\n",
                "    val_loss = validate(model, val_loader, criterion, device)\n",
                "    scheduler.step()\n",
                "    \n",
                "    history['loss'].append(stats['loss'])\n",
                "    history['val_loss'].append(val_loss)\n",
                "    history['hm'].append(stats['hm'])\n",
                "    history['size'].append(stats['size'])\n",
                "    history['offset'].append(stats['offset'])\n",
                "    \n",
                "    print(f\"   Loss: {stats['loss']:.4f} (HM:{stats['hm']:.3f} Size:{stats['size']:.3f} Off:{stats['offset']:.3f})\")\n",
                "    print(f\"   Val Loss: {val_loss:.4f}\")\n",
                "    \n",
                "    if val_loss < best_loss:\n",
                "        best_loss = val_loss\n",
                "        torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'best_loss': best_loss}, \n",
                "                   'detection_model.pt')\n",
                "        print(\"   ‚≠ê Best model saved!\")\n",
                "\n",
                "print(f\"\\n‚úÖ Training complete! Best loss: {best_loss:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot training curves\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
                "\n",
                "axes[0].plot(history['loss'], label='Train')\n",
                "axes[0].plot(history['val_loss'], label='Val')\n",
                "axes[0].set_xlabel('Epoch')\n",
                "axes[0].set_ylabel('Loss')\n",
                "axes[0].legend()\n",
                "axes[0].set_title('Total Loss')\n",
                "axes[0].grid(True)\n",
                "\n",
                "axes[1].plot(history['hm'], label='Heatmap')\n",
                "axes[1].set_xlabel('Epoch')\n",
                "axes[1].set_ylabel('Loss')\n",
                "axes[1].set_title('Heatmap Loss')\n",
                "axes[1].grid(True)\n",
                "\n",
                "axes[2].plot(history['size'], label='Size')\n",
                "axes[2].plot(history['offset'], label='Offset')\n",
                "axes[2].set_xlabel('Epoch')\n",
                "axes[2].set_ylabel('Loss')\n",
                "axes[2].legend()\n",
                "axes[2].set_title('Regression Losses')\n",
                "axes[2].grid(True)\n",
                "\n",
                "plt.suptitle('MOUAADNET-ULTRA Detection Training', fontsize=14)\n",
                "plt.tight_layout()\n",
                "plt.savefig('detection_training.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6Ô∏è‚É£ Test Detection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test detection on sample\n",
                "model.eval()\n",
                "\n",
                "batch = next(iter(val_loader))\n",
                "images = batch['image'].to(device)\n",
                "\n",
                "with torch.no_grad():\n",
                "    outputs = model(images[:4])\n",
                "\n",
                "# Visualize\n",
                "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
                "\n",
                "for i in range(4):\n",
                "    # Input image\n",
                "    img = batch['image'][i].permute(1, 2, 0).cpu().numpy()\n",
                "    img = img * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]\n",
                "    img = np.clip(img, 0, 1)\n",
                "    axes[0, i].imshow(img)\n",
                "    axes[0, i].set_title('Input Image')\n",
                "    axes[0, i].axis('off')\n",
                "    \n",
                "    # Predicted heatmap\n",
                "    hm = torch.sigmoid(outputs['heatmaps'][0][i, 0]).cpu().numpy()\n",
                "    axes[1, i].imshow(hm, cmap='hot')\n",
                "    axes[1, i].set_title(f'Predicted (max: {hm.max():.2f})')\n",
                "    axes[1, i].axis('off')\n",
                "\n",
                "plt.suptitle('Detection Results', fontsize=14)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7Ô∏è‚É£ Export"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load best and export\n",
                "ckpt = torch.load('detection_model.pt')\n",
                "model.load_state_dict(ckpt['model_state_dict'])\n",
                "model.eval()\n",
                "model.fuse_for_inference()\n",
                "model.cpu()\n",
                "\n",
                "torch.onnx.export(model, torch.randn(1, 3, 416, 416), 'detection_model.onnx', input_names=['image'], opset_version=12)\n",
                "print(f\"‚úÖ Exported! Best loss: {ckpt['best_loss']:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import files\n",
                "files.download('detection_model.pt')\n",
                "files.download('detection_model.onnx')\n",
                "files.download('detection_training.png')\n",
                "print(\"üéâ Done! Now test with webcam_demo.py --weights detection_model.pt\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}