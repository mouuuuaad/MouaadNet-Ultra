{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üöÄ MOUAADNET-ULTRA Training\n",
                "## Human Detection & Gender Classification with PA-100k\n",
                "\n",
                "**Lead Architect:** MOUAAD IDOUFKIR\n",
                "\n",
                "[![GitHub](https://img.shields.io/badge/GitHub-MouaadNet--Ultra-blue)](https://github.com/mouuuuaad/MouaadNet-Ultra)\n",
                "\n",
                "---\n",
                "\n",
                "### üìã Steps:\n",
                "1. ‚úÖ Setup environment & GPU\n",
                "2. ‚úÖ Download PA-100k from Kaggle (automatic)\n",
                "3. ‚úÖ Prepare data loaders\n",
                "4. ‚úÖ Train model with mixed precision\n",
                "5. ‚úÖ Export to ONNX"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Environment Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check GPU\n",
                "!nvidia-smi"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clone MOUAADNET-ULTRA repository\n",
                "!git clone https://github.com/mouuuuaad/MouaadNet-Ultra.git\n",
                "%cd MouaadNet-Ultra\n",
                "\n",
                "# Install dependencies\n",
                "!pip install -q torch torchvision tqdm scipy kagglehub"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Download PA-100k Dataset from Kaggle"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import kagglehub\n",
                "\n",
                "# Download PA-100k dataset automatically\n",
                "print(\"üì• Downloading PA-100k dataset from Kaggle...\")\n",
                "DATA_PATH = kagglehub.dataset_download(\"yuulind/pa-100k\")\n",
                "\n",
                "print(f\"\\n‚úÖ Dataset downloaded to: {DATA_PATH}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Explore dataset structure\n",
                "import os\n",
                "\n",
                "print(\"üìÅ Dataset contents:\")\n",
                "for root, dirs, files in os.walk(DATA_PATH):\n",
                "    level = root.replace(DATA_PATH, '').count(os.sep)\n",
                "    indent = ' ' * 2 * level\n",
                "    print(f'{indent}{os.path.basename(root)}/')\n",
                "    subindent = ' ' * 2 * (level + 1)\n",
                "    for file in files[:5]:  # Show first 5 files\n",
                "        print(f'{subindent}{file}')\n",
                "    if len(files) > 5:\n",
                "        print(f'{subindent}... and {len(files) - 5} more files')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Dataset & DataLoader"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from torchvision import transforms\n",
                "from PIL import Image\n",
                "from scipy.io import loadmat\n",
                "from tqdm import tqdm\n",
                "\n",
                "class PA100kDataset(Dataset):\n",
                "    \"\"\"\n",
                "    PA-100k Dataset for Human Detection & Gender Classification.\n",
                "    \n",
                "    PA-100k contains cropped pedestrian images with 26 attributes.\n",
                "    Attribute indices:\n",
                "      - 0: Female\n",
                "      - 1: AgeOver60\n",
                "      - 2: Age18-60\n",
                "      - 3: AgeLess18\n",
                "      ... (more attributes)\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, root_dir, split='train', img_size=416, transform=None):\n",
                "        self.root_dir = root_dir\n",
                "        self.img_size = img_size\n",
                "        self.transform = transform or self._default_transform()\n",
                "        self.split = split\n",
                "        \n",
                "        # Find annotation file\n",
                "        self.anno_path = self._find_annotation()\n",
                "        self.img_dir = self._find_images()\n",
                "        \n",
                "        # Load annotations\n",
                "        if self.anno_path:\n",
                "            self._load_annotations()\n",
                "        else:\n",
                "            self._load_from_directory()\n",
                "        \n",
                "        print(f\"‚úÖ Loaded {len(self.images)} images for {split} split\")\n",
                "    \n",
                "    def _find_annotation(self):\n",
                "        \"\"\"Find annotation.mat file.\"\"\"\n",
                "        for root, dirs, files in os.walk(self.root_dir):\n",
                "            for f in files:\n",
                "                if f.endswith('.mat'):\n",
                "                    return os.path.join(root, f)\n",
                "        return None\n",
                "    \n",
                "    def _find_images(self):\n",
                "        \"\"\"Find images directory.\"\"\"\n",
                "        for root, dirs, files in os.walk(self.root_dir):\n",
                "            # Check if this dir has images\n",
                "            img_files = [f for f in files if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
                "            if len(img_files) > 100:  # PA-100k has 100k images\n",
                "                return root\n",
                "            # Check subdirs\n",
                "            for d in dirs:\n",
                "                if 'image' in d.lower() or 'data' in d.lower():\n",
                "                    return os.path.join(root, d)\n",
                "        return self.root_dir\n",
                "    \n",
                "    def _load_annotations(self):\n",
                "        \"\"\"Load from annotation.mat file.\"\"\"\n",
                "        anno = loadmat(self.anno_path)\n",
                "        \n",
                "        # Try different annotation formats\n",
                "        if self.split == 'train':\n",
                "            key_images = 'train_images_name'\n",
                "            key_labels = 'train_label'\n",
                "        elif self.split == 'val':\n",
                "            key_images = 'val_images_name'\n",
                "            key_labels = 'val_label'\n",
                "        else:\n",
                "            key_images = 'test_images_name'\n",
                "            key_labels = 'test_label'\n",
                "        \n",
                "        if key_images in anno:\n",
                "            self.images = [str(x[0][0]) for x in anno[key_images]]\n",
                "            self.labels = anno[key_labels]\n",
                "        else:\n",
                "            # Alternative format\n",
                "            self._load_from_directory()\n",
                "    \n",
                "    def _load_from_directory(self):\n",
                "        \"\"\"Fallback: load all images from directory.\"\"\"\n",
                "        all_images = []\n",
                "        for f in os.listdir(self.img_dir):\n",
                "            if f.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
                "                all_images.append(f)\n",
                "        \n",
                "        all_images = sorted(all_images)\n",
                "        n = len(all_images)\n",
                "        \n",
                "        # Split 80/10/10\n",
                "        if self.split == 'train':\n",
                "            self.images = all_images[:int(0.8 * n)]\n",
                "        elif self.split == 'val':\n",
                "            self.images = all_images[int(0.8 * n):int(0.9 * n)]\n",
                "        else:\n",
                "            self.images = all_images[int(0.9 * n):]\n",
                "        \n",
                "        self.labels = None\n",
                "    \n",
                "    def _default_transform(self):\n",
                "        return transforms.Compose([\n",
                "            transforms.Resize((self.img_size, self.img_size)),\n",
                "            transforms.ToTensor(),\n",
                "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
                "        ])\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.images)\n",
                "    \n",
                "    def _generate_heatmap(self, h, w):\n",
                "        \"\"\"Generate Gaussian heatmap centered on image.\"\"\"\n",
                "        cx, cy = w // 2, h // 2\n",
                "        sigma = min(h, w) // 6\n",
                "        x = np.arange(w)\n",
                "        y = np.arange(h)\n",
                "        xx, yy = np.meshgrid(x, y)\n",
                "        heatmap = np.exp(-((xx - cx)**2 + (yy - cy)**2) / (2 * sigma**2))\n",
                "        return heatmap.astype(np.float32)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        # Load image\n",
                "        img_name = self.images[idx]\n",
                "        img_path = os.path.join(self.img_dir, img_name)\n",
                "        \n",
                "        try:\n",
                "            image = Image.open(img_path).convert('RGB')\n",
                "        except:\n",
                "            # Return random tensor if image fails\n",
                "            return self.__getitem__((idx + 1) % len(self))\n",
                "        \n",
                "        if self.transform:\n",
                "            image = self.transform(image)\n",
                "        \n",
                "        # Generate targets\n",
                "        hm_size = self.img_size // 4\n",
                "        heatmap = self._generate_heatmap(hm_size, hm_size)\n",
                "        heatmap = torch.from_numpy(heatmap).unsqueeze(0)\n",
                "        \n",
                "        # Size (person fills ~80% of frame)\n",
                "        size = torch.tensor([0.8, 0.9])\n",
                "        \n",
                "        # Offset\n",
                "        offset = torch.tensor([0.0, 0.0])\n",
                "        \n",
                "        # Gender: attribute 0 = Female\n",
                "        if self.labels is not None:\n",
                "            label = self.labels[idx]\n",
                "            gender = 1.0 - float(label[0])  # 0=Female->1, 1=Female->0 (Male=1)\n",
                "        else:\n",
                "            gender = 0.5\n",
                "        \n",
                "        gender = torch.tensor([gender], dtype=torch.float32)\n",
                "        \n",
                "        return {\n",
                "            'image': image,\n",
                "            'heatmap': heatmap,\n",
                "            'size': size,\n",
                "            'offset': offset,\n",
                "            'gender': gender,\n",
                "        }\n",
                "\n",
                "\n",
                "def create_dataloaders(data_dir, batch_size=32, img_size=416, num_workers=2):\n",
                "    \"\"\"Create train and validation dataloaders.\"\"\"\n",
                "    \n",
                "    train_transform = transforms.Compose([\n",
                "        transforms.Resize((img_size + 32, img_size + 32)),\n",
                "        transforms.RandomCrop(img_size),\n",
                "        transforms.RandomHorizontalFlip(p=0.5),\n",
                "        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
                "        transforms.ToTensor(),\n",
                "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
                "    ])\n",
                "    \n",
                "    val_transform = transforms.Compose([\n",
                "        transforms.Resize((img_size, img_size)),\n",
                "        transforms.ToTensor(),\n",
                "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
                "    ])\n",
                "    \n",
                "    train_dataset = PA100kDataset(data_dir, split='train', img_size=img_size, transform=train_transform)\n",
                "    val_dataset = PA100kDataset(data_dir, split='val', img_size=img_size, transform=val_transform)\n",
                "    \n",
                "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
                "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
                "    \n",
                "    return train_loader, val_loader\n",
                "\n",
                "print(\"‚úÖ Dataset classes defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create dataloaders using downloaded dataset\n",
                "BATCH_SIZE = 32\n",
                "IMG_SIZE = 416\n",
                "\n",
                "train_loader, val_loader = create_dataloaders(\n",
                "    DATA_PATH,\n",
                "    batch_size=BATCH_SIZE,\n",
                "    img_size=IMG_SIZE,\n",
                "    num_workers=2\n",
                ")\n",
                "\n",
                "print(f\"\\nüìä Data Summary:\")\n",
                "print(f\"   Train batches: {len(train_loader)}\")\n",
                "print(f\"   Val batches: {len(val_loader)}\")\n",
                "print(f\"   Batch size: {BATCH_SIZE}\")\n",
                "print(f\"   Image size: {IMG_SIZE}x{IMG_SIZE}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize samples\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "batch = next(iter(train_loader))\n",
                "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
                "\n",
                "for i in range(4):\n",
                "    img = batch['image'][i].permute(1, 2, 0).numpy()\n",
                "    img = img * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]\n",
                "    img = np.clip(img, 0, 1)\n",
                "    \n",
                "    axes[0, i].imshow(img)\n",
                "    gender = 'Male' if batch['gender'][i] > 0.5 else 'Female'\n",
                "    axes[0, i].set_title(f'Gender: {gender}')\n",
                "    axes[0, i].axis('off')\n",
                "    \n",
                "    hm = batch['heatmap'][i, 0].numpy()\n",
                "    axes[1, i].imshow(hm, cmap='hot')\n",
                "    axes[1, i].set_title('Detection Heatmap')\n",
                "    axes[1, i].axis('off')\n",
                "\n",
                "plt.suptitle('PA-100k Training Samples', fontsize=14)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4Ô∏è‚É£ Model & Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.insert(0, '.')\n",
                "\n",
                "from mouaadnet_ultra.model import MouaadNetUltra\n",
                "from mouaadnet_ultra.losses import MultiTaskLoss\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"üñ•Ô∏è Device: {device}\")\n",
                "\n",
                "model = MouaadNetUltra()\n",
                "model = model.to(device)\n",
                "\n",
                "print(f\"\\nüìä Model Info:\")\n",
                "print(f\"   Parameters: {model.count_parameters():,}\")\n",
                "print(f\"   FP32 Size: {model.get_model_size_mb():.2f} MB\")\n",
                "print(f\"   INT8 Size: {model.get_model_size_mb('int8'):.2f} MB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training configuration\n",
                "EPOCHS = 30\n",
                "LR = 1e-3\n",
                "WEIGHT_DECAY = 1e-4\n",
                "\n",
                "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
                "\n",
                "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
                "    optimizer, max_lr=LR * 10, epochs=EPOCHS,\n",
                "    steps_per_epoch=len(train_loader), pct_start=0.3\n",
                ")\n",
                "\n",
                "criterion = MultiTaskLoss(det_weight=1.0, gender_weight=1.0, gender_pos_weight=3.0)\n",
                "scaler = torch.cuda.amp.GradScaler()\n",
                "\n",
                "print(\"‚úÖ Training setup complete\")\n",
                "print(f\"   Epochs: {EPOCHS}\")\n",
                "print(f\"   Learning rate: {LR} ‚Üí {LR * 10}\")\n",
                "print(f\"   Optimizer: AdamW\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_epoch(model, loader, optimizer, scheduler, criterion, scaler, device):\n",
                "    model.train()\n",
                "    total_loss = 0\n",
                "    \n",
                "    pbar = tqdm(loader, desc='Training')\n",
                "    for batch in pbar:\n",
                "        images = batch['image'].to(device)\n",
                "        B = images.shape[0]\n",
                "        hm_h, hm_w = images.shape[2] // 4, images.shape[3] // 4\n",
                "        \n",
                "        targets = {\n",
                "            'heatmaps': [batch['heatmap'].to(device)],\n",
                "            'sizes': [batch['size'].view(B, 2, 1, 1).expand(B, 2, hm_h, hm_w).to(device)],\n",
                "            'offsets': [batch['offset'].view(B, 2, 1, 1).expand(B, 2, hm_h, hm_w).to(device)],\n",
                "            'gender_labels': batch['gender'].to(device),\n",
                "        }\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        \n",
                "        with torch.cuda.amp.autocast():\n",
                "            outputs = model(images)\n",
                "            predictions = {\n",
                "                'heatmaps': [outputs['heatmaps'][0]],\n",
                "                'sizes': [outputs['sizes'][0]],\n",
                "                'offsets': [outputs['offsets'][0]],\n",
                "                'gender': outputs['gender'],\n",
                "            }\n",
                "            losses = criterion(predictions, targets)\n",
                "            loss = losses['total']\n",
                "        \n",
                "        scaler.scale(loss).backward()\n",
                "        scaler.unscale_(optimizer)\n",
                "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
                "        scaler.step(optimizer)\n",
                "        scaler.update()\n",
                "        scheduler.step()\n",
                "        \n",
                "        total_loss += loss.item()\n",
                "        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'lr': f'{scheduler.get_last_lr()[0]:.6f}'})\n",
                "    \n",
                "    return total_loss / len(loader)\n",
                "\n",
                "\n",
                "@torch.no_grad()\n",
                "def validate(model, loader, criterion, device):\n",
                "    model.eval()\n",
                "    total_loss = 0\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    \n",
                "    for batch in tqdm(loader, desc='Validating'):\n",
                "        images = batch['image'].to(device)\n",
                "        B = images.shape[0]\n",
                "        hm_h, hm_w = images.shape[2] // 4, images.shape[3] // 4\n",
                "        \n",
                "        targets = {\n",
                "            'heatmaps': [batch['heatmap'].to(device)],\n",
                "            'sizes': [batch['size'].view(B, 2, 1, 1).expand(B, 2, hm_h, hm_w).to(device)],\n",
                "            'offsets': [batch['offset'].view(B, 2, 1, 1).expand(B, 2, hm_h, hm_w).to(device)],\n",
                "            'gender_labels': batch['gender'].to(device),\n",
                "        }\n",
                "        \n",
                "        with torch.cuda.amp.autocast():\n",
                "            outputs = model(images)\n",
                "            predictions = {\n",
                "                'heatmaps': [outputs['heatmaps'][0]],\n",
                "                'sizes': [outputs['sizes'][0]],\n",
                "                'offsets': [outputs['offsets'][0]],\n",
                "                'gender': outputs['gender'],\n",
                "            }\n",
                "            losses = criterion(predictions, targets)\n",
                "        \n",
                "        total_loss += losses['total'].item()\n",
                "        \n",
                "        gender_pred = (torch.sigmoid(outputs['gender']) > 0.5).float()\n",
                "        correct += (gender_pred == batch['gender'].to(device)).sum().item()\n",
                "        total += batch['gender'].size(0)\n",
                "    \n",
                "    return total_loss / len(loader), correct / total * 100\n",
                "\n",
                "print(\"‚úÖ Training functions ready\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# üöÄ TRAIN!\n",
                "best_loss = float('inf')\n",
                "history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"üöÄ Starting Training\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "for epoch in range(EPOCHS):\n",
                "    print(f\"\\nüìç Epoch {epoch+1}/{EPOCHS}\")\n",
                "    \n",
                "    train_loss = train_epoch(model, train_loader, optimizer, scheduler, criterion, scaler, device)\n",
                "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
                "    \n",
                "    history['train_loss'].append(train_loss)\n",
                "    history['val_loss'].append(val_loss)\n",
                "    history['val_acc'].append(val_acc)\n",
                "    \n",
                "    print(f\"   Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Gender Acc: {val_acc:.2f}%\")\n",
                "    \n",
                "    if val_loss < best_loss:\n",
                "        best_loss = val_loss\n",
                "        torch.save({\n",
                "            'epoch': epoch,\n",
                "            'model_state_dict': model.state_dict(),\n",
                "            'optimizer_state_dict': optimizer.state_dict(),\n",
                "            'best_loss': best_loss,\n",
                "            'val_acc': val_acc,\n",
                "        }, 'best_model.pt')\n",
                "        print(\"   ‚≠ê New best model saved!\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"‚úÖ Training Complete!\")\n",
                "print(f\"   Best Val Loss: {best_loss:.4f}\")\n",
                "print(f\"   Final Gender Accuracy: {history['val_acc'][-1]:.2f}%\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot training curves\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "ax1.plot(history['train_loss'], label='Train', linewidth=2)\n",
                "ax1.plot(history['val_loss'], label='Validation', linewidth=2)\n",
                "ax1.set_xlabel('Epoch')\n",
                "ax1.set_ylabel('Loss')\n",
                "ax1.set_title('Training & Validation Loss')\n",
                "ax1.legend()\n",
                "ax1.grid(True, alpha=0.3)\n",
                "\n",
                "ax2.plot(history['val_acc'], color='green', linewidth=2)\n",
                "ax2.set_xlabel('Epoch')\n",
                "ax2.set_ylabel('Accuracy (%)')\n",
                "ax2.set_title('Gender Classification Accuracy')\n",
                "ax2.grid(True, alpha=0.3)\n",
                "\n",
                "plt.suptitle('MOUAADNET-ULTRA Training on PA-100k', fontsize=14)\n",
                "plt.tight_layout()\n",
                "plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5Ô∏è‚É£ Export Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load best model\n",
                "checkpoint = torch.load('best_model.pt')\n",
                "model.load_state_dict(checkpoint['model_state_dict'])\n",
                "model.eval()\n",
                "\n",
                "print(f\"‚úÖ Loaded best model from epoch {checkpoint['epoch'] + 1}\")\n",
                "print(f\"   Val Loss: {checkpoint['best_loss']:.4f}\")\n",
                "print(f\"   Gender Accuracy: {checkpoint['val_acc']:.2f}%\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Fuse for faster inference\n",
                "model.fuse_for_inference()\n",
                "print(\"‚úÖ Model fused for inference\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Export to ONNX\n",
                "model.cpu()\n",
                "dummy_input = torch.randn(1, 3, 416, 416)\n",
                "\n",
                "torch.onnx.export(\n",
                "    model, dummy_input,\n",
                "    'mouaadnet_ultra_pa100k.onnx',\n",
                "    input_names=['image'],\n",
                "    output_names=['heatmaps', 'sizes', 'offsets', 'gender'],\n",
                "    dynamic_axes={'image': {0: 'batch'}},\n",
                "    opset_version=12,\n",
                ")\n",
                "\n",
                "import os\n",
                "onnx_size = os.path.getsize('mouaadnet_ultra_pa100k.onnx') / (1024 * 1024)\n",
                "print(f\"‚úÖ Exported to ONNX\")\n",
                "print(f\"   File: mouaadnet_ultra_pa100k.onnx\")\n",
                "print(f\"   Size: {onnx_size:.2f} MB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Download trained files\n",
                "from google.colab import files\n",
                "\n",
                "print(\"üì• Downloading trained files...\")\n",
                "files.download('best_model.pt')\n",
                "files.download('mouaadnet_ultra_pa100k.onnx')\n",
                "files.download('training_curves.png')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üéâ Done!\n",
                "\n",
                "Your MOUAADNET-ULTRA model is now trained! \n",
                "\n",
                "### Downloaded Files:\n",
                "- `best_model.pt` - PyTorch checkpoint\n",
                "- `mouaadnet_ultra_pa100k.onnx` - ONNX model for deployment\n",
                "- `training_curves.png` - Training visualization\n",
                "\n",
                "### Next Steps:\n",
                "1. Copy `best_model.pt` to your local project\n",
                "2. Run webcam demo with trained weights:\n",
                "   ```bash\n",
                "   python examples/webcam_demo.py --weights best_model.pt\n",
                "   ```"
            ]
        }
    ]
}