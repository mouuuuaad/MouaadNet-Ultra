{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üöÄ MOUAADNET-ULTRA Training\n",
                "## Human Detection & Gender Classification with PA-100k\n",
                "\n",
                "**Lead Architect:** MOUAAD IDOUFKIR\n",
                "\n",
                "[![GitHub](https://img.shields.io/badge/GitHub-MouaadNet--Ultra-blue)](https://github.com/mouuuuaad/MouaadNet-Ultra)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Environment Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!nvidia-smi"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!git clone https://github.com/mouuuuaad/MouaadNet-Ultra.git\n",
                "%cd MouaadNet-Ultra\n",
                "!pip install -q torch torchvision tqdm scipy kagglehub"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Download PA-100k Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import kagglehub\n",
                "\n",
                "print(\"üì• Downloading PA-100k dataset...\")\n",
                "DATA_PATH = kagglehub.dataset_download(\"yuulind/pa-100k\")\n",
                "print(f\"‚úÖ Dataset: {DATA_PATH}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Dataset & DataLoader"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from torchvision import transforms\n",
                "from PIL import Image\n",
                "from scipy.io import loadmat\n",
                "from tqdm import tqdm\n",
                "\n",
                "class PA100kDataset(Dataset):\n",
                "    def __init__(self, root_dir, split='train', img_size=416, transform=None):\n",
                "        self.root_dir = root_dir\n",
                "        self.img_size = img_size\n",
                "        self.transform = transform or self._default_transform()\n",
                "        self.split = split\n",
                "        \n",
                "        self.anno_path = self._find_file('.mat')\n",
                "        self.img_dir = self._find_images()\n",
                "        self._load_data()\n",
                "        print(f\"‚úÖ {split}: {len(self.images)} images\")\n",
                "    \n",
                "    def _find_file(self, ext):\n",
                "        for root, _, files in os.walk(self.root_dir):\n",
                "            for f in files:\n",
                "                if f.endswith(ext):\n",
                "                    return os.path.join(root, f)\n",
                "        return None\n",
                "    \n",
                "    def _find_images(self):\n",
                "        for root, dirs, files in os.walk(self.root_dir):\n",
                "            imgs = [f for f in files if f.lower().endswith(('.jpg', '.png'))]\n",
                "            if len(imgs) > 100:\n",
                "                return root\n",
                "        return self.root_dir\n",
                "    \n",
                "    def _load_data(self):\n",
                "        if self.anno_path:\n",
                "            anno = loadmat(self.anno_path)\n",
                "            key = f'{self.split}_images_name'\n",
                "            if key in anno:\n",
                "                self.images = [str(x[0][0]) for x in anno[key]]\n",
                "                self.labels = anno[f'{self.split}_label']\n",
                "                return\n",
                "        \n",
                "        all_imgs = sorted([f for f in os.listdir(self.img_dir) if f.lower().endswith(('.jpg', '.png'))])\n",
                "        n = len(all_imgs)\n",
                "        if self.split == 'train':\n",
                "            self.images = all_imgs[:int(0.8*n)]\n",
                "        elif self.split == 'val':\n",
                "            self.images = all_imgs[int(0.8*n):int(0.9*n)]\n",
                "        else:\n",
                "            self.images = all_imgs[int(0.9*n):]\n",
                "        self.labels = None\n",
                "    \n",
                "    def _default_transform(self):\n",
                "        return transforms.Compose([\n",
                "            transforms.Resize((self.img_size, self.img_size)),\n",
                "            transforms.ToTensor(),\n",
                "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
                "        ])\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.images)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        img_path = os.path.join(self.img_dir, self.images[idx])\n",
                "        try:\n",
                "            image = Image.open(img_path).convert('RGB')\n",
                "        except:\n",
                "            return self.__getitem__((idx + 1) % len(self))\n",
                "        \n",
                "        if self.transform:\n",
                "            image = self.transform(image)\n",
                "        \n",
                "        # Heatmap (person centered)\n",
                "        hm_size = self.img_size // 4\n",
                "        cx, cy = hm_size // 2, hm_size // 2\n",
                "        sigma = hm_size // 6\n",
                "        x = np.arange(hm_size)\n",
                "        y = np.arange(hm_size)\n",
                "        xx, yy = np.meshgrid(x, y)\n",
                "        heatmap = np.exp(-((xx - cx)**2 + (yy - cy)**2) / (2 * sigma**2 + 1e-6))\n",
                "        heatmap = torch.from_numpy(heatmap.astype(np.float32)).unsqueeze(0)\n",
                "        \n",
                "        # Gender\n",
                "        if self.labels is not None:\n",
                "            gender = 1.0 - float(self.labels[idx][0])  # Female=0 -> Male=1\n",
                "        else:\n",
                "            gender = 0.5\n",
                "        \n",
                "        return {\n",
                "            'image': image,\n",
                "            'heatmap': heatmap,\n",
                "            'gender': torch.tensor([gender], dtype=torch.float32),\n",
                "        }\n",
                "\n",
                "\n",
                "def create_dataloaders(data_dir, batch_size=32, img_size=416):\n",
                "    train_tf = transforms.Compose([\n",
                "        transforms.Resize((img_size + 32, img_size + 32)),\n",
                "        transforms.RandomCrop(img_size),\n",
                "        transforms.RandomHorizontalFlip(0.5),\n",
                "        transforms.ColorJitter(0.2, 0.2, 0.2),\n",
                "        transforms.ToTensor(),\n",
                "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
                "    ])\n",
                "    val_tf = transforms.Compose([\n",
                "        transforms.Resize((img_size, img_size)),\n",
                "        transforms.ToTensor(),\n",
                "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
                "    ])\n",
                "    \n",
                "    train_ds = PA100kDataset(data_dir, 'train', img_size, train_tf)\n",
                "    val_ds = PA100kDataset(data_dir, 'val', img_size, val_tf)\n",
                "    \n",
                "    train_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
                "    val_loader = DataLoader(val_ds, batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
                "    \n",
                "    return train_loader, val_loader\n",
                "\n",
                "print(\"‚úÖ Dataset ready\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "BATCH_SIZE = 32\n",
                "IMG_SIZE = 416\n",
                "\n",
                "train_loader, val_loader = create_dataloaders(DATA_PATH, BATCH_SIZE, IMG_SIZE)\n",
                "print(f\"Train: {len(train_loader)} batches | Val: {len(val_loader)} batches\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4Ô∏è‚É£ Model & Stable Loss Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.insert(0, '.')\n",
                "\n",
                "from mouaadnet_ultra.model import MouaadNetUltra\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Device: {device}\")\n",
                "\n",
                "model = MouaadNetUltra()\n",
                "model = model.to(device)\n",
                "print(f\"Parameters: {model.count_parameters():,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ‚ö†Ô∏è STABLE LOSS FUNCTIONS (No NaN!)\n",
                "\n",
                "class StableFocalLoss(nn.Module):\n",
                "    \"\"\"Numerically stable focal loss for heatmaps.\"\"\"\n",
                "    def __init__(self, alpha=2.0, beta=4.0):\n",
                "        super().__init__()\n",
                "        self.alpha = alpha\n",
                "        self.beta = beta\n",
                "    \n",
                "    def forward(self, pred, target):\n",
                "        # Clamp predictions to avoid log(0)\n",
                "        pred = torch.clamp(pred, min=1e-6, max=1-1e-6)\n",
                "        \n",
                "        pos_mask = target.eq(1).float()\n",
                "        neg_mask = target.lt(1).float()\n",
                "        \n",
                "        # Positive loss\n",
                "        pos_loss = -torch.log(pred) * torch.pow(1 - pred, self.alpha) * pos_mask\n",
                "        \n",
                "        # Negative loss with reduced weight near positives\n",
                "        neg_weight = torch.pow(1 - target, self.beta)\n",
                "        neg_loss = -torch.log(1 - pred) * torch.pow(pred, self.alpha) * neg_weight * neg_mask\n",
                "        \n",
                "        num_pos = pos_mask.sum().clamp(min=1)\n",
                "        loss = (pos_loss.sum() + neg_loss.sum()) / num_pos\n",
                "        \n",
                "        return loss\n",
                "\n",
                "\n",
                "class StableMultiTaskLoss(nn.Module):\n",
                "    \"\"\"Stable multi-task loss.\"\"\"\n",
                "    def __init__(self, hm_weight=1.0, gender_weight=1.0):\n",
                "        super().__init__()\n",
                "        self.hm_weight = hm_weight\n",
                "        self.gender_weight = gender_weight\n",
                "        self.focal = StableFocalLoss()\n",
                "    \n",
                "    def forward(self, pred_hm, target_hm, pred_gender, target_gender):\n",
                "        # Heatmap loss\n",
                "        pred_hm = torch.sigmoid(pred_hm)  # Ensure [0, 1]\n",
                "        hm_loss = self.focal(pred_hm, target_hm)\n",
                "        \n",
                "        # Gender loss (stable BCE)\n",
                "        gender_loss = F.binary_cross_entropy_with_logits(\n",
                "            pred_gender, target_gender, \n",
                "            pos_weight=torch.tensor([3.0], device=pred_gender.device)\n",
                "        )\n",
                "        \n",
                "        total = self.hm_weight * hm_loss + self.gender_weight * gender_loss\n",
                "        \n",
                "        return {\n",
                "            'total': total,\n",
                "            'hm_loss': hm_loss,\n",
                "            'gender_loss': gender_loss,\n",
                "        }\n",
                "\n",
                "print(\"‚úÖ Stable loss functions ready\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training config\n",
                "EPOCHS = 30\n",
                "LR = 5e-4  # Lower LR for stability\n",
                "\n",
                "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
                "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-6)\n",
                "criterion = StableMultiTaskLoss(hm_weight=1.0, gender_weight=1.0)\n",
                "scaler = torch.amp.GradScaler('cuda')\n",
                "\n",
                "print(f\"‚úÖ Training config: {EPOCHS} epochs, LR={LR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_epoch(model, loader, optimizer, criterion, scaler, device):\n",
                "    model.train()\n",
                "    total_loss = 0\n",
                "    total_hm = 0\n",
                "    total_gender = 0\n",
                "    \n",
                "    pbar = tqdm(loader, desc='Training')\n",
                "    for batch in pbar:\n",
                "        images = batch['image'].to(device)\n",
                "        heatmaps = batch['heatmap'].to(device)\n",
                "        genders = batch['gender'].to(device)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        \n",
                "        with torch.amp.autocast('cuda'):\n",
                "            outputs = model(images)\n",
                "            \n",
                "            # Use first scale heatmap\n",
                "            pred_hm = outputs['heatmaps'][0]\n",
                "            pred_gender = outputs['gender']\n",
                "            \n",
                "            losses = criterion(pred_hm, heatmaps, pred_gender, genders)\n",
                "            loss = losses['total']\n",
                "        \n",
                "        # Check for NaN\n",
                "        if torch.isnan(loss) or torch.isinf(loss):\n",
                "            print(\"‚ö†Ô∏è NaN detected, skipping batch\")\n",
                "            optimizer.zero_grad()\n",
                "            continue\n",
                "        \n",
                "        scaler.scale(loss).backward()\n",
                "        \n",
                "        # Gradient clipping\n",
                "        scaler.unscale_(optimizer)\n",
                "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
                "        \n",
                "        scaler.step(optimizer)\n",
                "        scaler.update()\n",
                "        \n",
                "        total_loss += loss.item()\n",
                "        total_hm += losses['hm_loss'].item()\n",
                "        total_gender += losses['gender_loss'].item()\n",
                "        \n",
                "        pbar.set_postfix({\n",
                "            'loss': f\"{loss.item():.4f}\",\n",
                "            'hm': f\"{losses['hm_loss'].item():.4f}\",\n",
                "            'gender': f\"{losses['gender_loss'].item():.4f}\"\n",
                "        })\n",
                "    \n",
                "    n = len(loader)\n",
                "    return total_loss/n, total_hm/n, total_gender/n\n",
                "\n",
                "\n",
                "@torch.no_grad()\n",
                "def validate(model, loader, criterion, device):\n",
                "    model.eval()\n",
                "    total_loss = 0\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    \n",
                "    for batch in tqdm(loader, desc='Validating'):\n",
                "        images = batch['image'].to(device)\n",
                "        heatmaps = batch['heatmap'].to(device)\n",
                "        genders = batch['gender'].to(device)\n",
                "        \n",
                "        with torch.amp.autocast('cuda'):\n",
                "            outputs = model(images)\n",
                "            losses = criterion(outputs['heatmaps'][0], heatmaps, outputs['gender'], genders)\n",
                "        \n",
                "        total_loss += losses['total'].item()\n",
                "        \n",
                "        pred = (torch.sigmoid(outputs['gender']) > 0.5).float()\n",
                "        correct += (pred == genders).sum().item()\n",
                "        total += genders.size(0)\n",
                "    \n",
                "    return total_loss / len(loader), correct / total * 100\n",
                "\n",
                "print(\"‚úÖ Training functions ready\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# üöÄ TRAIN!\n",
                "best_acc = 0\n",
                "history = {'loss': [], 'val_loss': [], 'acc': []}\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"üöÄ Training MOUAADNET-ULTRA\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "for epoch in range(EPOCHS):\n",
                "    print(f\"\\nüìç Epoch {epoch+1}/{EPOCHS}\")\n",
                "    \n",
                "    train_loss, hm_loss, gender_loss = train_epoch(model, train_loader, optimizer, criterion, scaler, device)\n",
                "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
                "    scheduler.step()\n",
                "    \n",
                "    history['loss'].append(train_loss)\n",
                "    history['val_loss'].append(val_loss)\n",
                "    history['acc'].append(val_acc)\n",
                "    \n",
                "    print(f\"   Loss: {train_loss:.4f} (HM: {hm_loss:.4f}, Gender: {gender_loss:.4f})\")\n",
                "    print(f\"   Val Loss: {val_loss:.4f} | Gender Acc: {val_acc:.2f}%\")\n",
                "    \n",
                "    if val_acc > best_acc:\n",
                "        best_acc = val_acc\n",
                "        torch.save({\n",
                "            'epoch': epoch,\n",
                "            'model_state_dict': model.state_dict(),\n",
                "            'best_acc': best_acc,\n",
                "        }, 'best_model.pt')\n",
                "        print(\"   ‚≠ê Best model saved!\")\n",
                "\n",
                "print(f\"\\n‚úÖ Training complete! Best accuracy: {best_acc:.2f}%\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot results\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
                "\n",
                "ax1.plot(history['loss'], label='Train')\n",
                "ax1.plot(history['val_loss'], label='Val')\n",
                "ax1.set_xlabel('Epoch')\n",
                "ax1.set_ylabel('Loss')\n",
                "ax1.legend()\n",
                "ax1.set_title('Loss')\n",
                "ax1.grid(True)\n",
                "\n",
                "ax2.plot(history['acc'], color='green')\n",
                "ax2.set_xlabel('Epoch')\n",
                "ax2.set_ylabel('Accuracy (%)')\n",
                "ax2.set_title('Gender Accuracy')\n",
                "ax2.grid(True)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('training.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5Ô∏è‚É£ Export"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load best & export\n",
                "ckpt = torch.load('best_model.pt')\n",
                "model.load_state_dict(ckpt['model_state_dict'])\n",
                "model.eval()\n",
                "model.fuse_for_inference()\n",
                "model.cpu()\n",
                "\n",
                "torch.onnx.export(\n",
                "    model, torch.randn(1, 3, 416, 416),\n",
                "    'mouaadnet_ultra.onnx',\n",
                "    input_names=['image'],\n",
                "    opset_version=12\n",
                ")\n",
                "\n",
                "print(f\"‚úÖ Exported: mouaadnet_ultra.onnx\")\n",
                "print(f\"   Best accuracy: {ckpt['best_acc']:.2f}%\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import files\n",
                "files.download('best_model.pt')\n",
                "files.download('mouaadnet_ultra.onnx')\n",
                "files.download('training.png')\n",
                "print(\"üéâ Done!\")"
            ]
        }
    ]
}