# MOUAADNET-ULTRA

<div align="center">

<img src="docs/assets/logo.png" alt="MOUAADNET-ULTRA Logo" width="200"/>

**High-Efficiency Human Detection and Gender Classification**

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 2.0+](https://img.shields.io/badge/pytorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

[ğŸ“– Documentation](docs/) | [ğŸš€ Quick Start](#quick-start) | [ğŸ“¦ Installation](#installation) | [ğŸ¤ Contributing](CONTRIBUTING.md)

</div>

---

## ğŸ¯ Key Features

- **Ultra-Lightweight**: 868K parameters, ~0.83MB INT8 quantized
- **Multi-Task**: Simultaneous human detection + gender classification
- **Real-Time**: Designed for <10ms inference on GPU
- **Production-Ready**: ONNX export, INT8 quantization, mobile deployment

## ğŸ“Š Model Variants

| Variant | Parameters | FP32 Size | INT8 Size | Use Case |
|---------|------------|-----------|-----------|----------|
| **Ultra** | 868,860 | 3.31 MB | 0.83 MB | Balanced |
| **Lite** | 517,411 | 1.97 MB | 0.49 MB | Mobile/Edge |
| **Pro** | ~1.5M | ~6 MB | ~1.5 MB | High Accuracy |

## ğŸ—ï¸ Architecture

```
Input (416Ã—416Ã—3)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  NANO-BACKBONE (5 Stages)           â”‚
â”‚  PConv + Ghost + IRB + ReLU6        â”‚
â”‚  16 â†’ 24 â†’ 40 â†’ 80 â†’ 128 channels   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ P3, P4, P5
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SLIM-PAN NECK                      â”‚
â”‚  Bi-directional Feature Fusion      â”‚
â”‚  + SPP-Lite + CSP Connections       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ N3, N4, N5
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DECOUPLED HEADS                    â”‚
â”‚  â”œâ”€ Detection: Heatmap + Size + Off â”‚
â”‚  â””â”€ Gender: Attention + GAP + FC    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Output: Bounding Boxes + Gender Labels
```

## ğŸš€ Quick Start

```python
import torch
from mouaadnet_ultra import MouaadNetUltra

# Load model
model = MouaadNetUltra()
model.eval()

# Inference
image = torch.randn(1, 3, 416, 416)
outputs = model(image)

# Results
print(f"Detection heatmaps: {[h.shape for h in outputs['heatmaps']]}")
print(f"Gender prediction: {torch.sigmoid(outputs['gender'])}")
```

## ğŸ“¦ Installation

### From Source (Recommended)
```bash
git clone https://github.com/mouaadidoufkir/mouaadnet-ultra.git
cd mouaadnet-ultra
pip install -e .
```

### Requirements
```bash
pip install -r requirements.txt
```

## ğŸ“ Training

```bash
# Train with default config
python scripts/train.py --config configs/default.yaml

# Train with custom dataset
python scripts/train.py \
    --data /path/to/dataset \
    --epochs 100 \
    --batch-size 32
```

## ğŸ“¤ Export

```bash
# Export to ONNX
python scripts/export.py --format onnx --output exports/model.onnx

# Export with INT8 quantization
python scripts/export.py --format onnx --quantize int8
```

## ğŸ“ Project Structure

```
mouaadnet-ultra/
â”œâ”€â”€ mouaadnet_ultra/          # Core library
â”‚   â”œâ”€â”€ backbone/             # Nano-backbone components
â”‚   â”œâ”€â”€ neck/                 # Feature fusion modules
â”‚   â”œâ”€â”€ heads/                # Detection & classification heads
â”‚   â”œâ”€â”€ losses/               # Loss functions
â”‚   â”œâ”€â”€ optim/                # Optimization utilities
â”‚   â””â”€â”€ model.py              # Main model class
â”œâ”€â”€ configs/                  # Configuration files
â”œâ”€â”€ data/                     # Dataset utilities
â”œâ”€â”€ docs/                     # Documentation
â”œâ”€â”€ examples/                 # Usage examples
â”œâ”€â”€ scripts/                  # Training & export scripts
â”œâ”€â”€ tests/                    # Test suite
â””â”€â”€ exports/                  # Exported models
```

## ğŸ§ª Testing

```bash
# Run all tests
python -m pytest tests/ -v

# Quick validation
python tests/test_all.py
```

## ğŸ“ˆ Benchmarks

| Hardware | Input Size | FP32 | FP16 | INT8 |
|----------|------------|------|------|------|
| RTX 3090 | 416Ã—416 | 4.2ms | 2.1ms | 1.3ms |
| Jetson Nano | 416Ã—416 | 45ms | 28ms | 18ms |
| CPU (i7) | 416Ã—416 | 120ms | - | 85ms |

## ğŸ¤ Contributing

We welcome contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## ğŸ“„ License

This project is licensed under the MIT License - see [LICENSE](LICENSE) for details.

## ğŸ‘¤ Author

**MOUAAD IDOUFKIR** - Lead Architect

## ğŸ™ Acknowledgments

- FasterNet for Partial Convolution inspiration
- GhostNet for Ghost Module design
- RepVGG for structural re-parameterization
- CenterNet for anchor-free detection

---

<div align="center">
Made with â¤ï¸ by MOUAAD IDOUFKIR
</div>
