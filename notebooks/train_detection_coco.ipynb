{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üéØ MOUAADNET-ULTRA: Human Detection Training\n",
                "\n",
                "**Author:** MOUAAD IDOUFKIR  \n",
                "**Platform:** Lightning AI / Google Colab\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# CELL 1: Check GPU & Install Dependencies\n",
                "# ============================================\n",
                "!nvidia-smi\n",
                "!pip install -q torch torchvision tqdm pycocotools opencv-python"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# CELL 2: Clone Repository\n",
                "# ============================================\n",
                "import os\n",
                "\n",
                "# Works on both Lightning AI and Colab\n",
                "if os.path.exists('/teamspace'):\n",
                "    # Lightning AI\n",
                "    WORK_DIR = '/teamspace/studios/this_studio'\n",
                "else:\n",
                "    # Colab\n",
                "    WORK_DIR = '/content'\n",
                "\n",
                "REPO_DIR = f'{WORK_DIR}/MouaadNet-Ultra'\n",
                "DATA_DIR = f'{WORK_DIR}/coco'\n",
                "CKPT_DIR = f'{WORK_DIR}/checkpoints'\n",
                "\n",
                "os.makedirs(DATA_DIR, exist_ok=True)\n",
                "os.makedirs(CKPT_DIR, exist_ok=True)\n",
                "\n",
                "if not os.path.exists(REPO_DIR):\n",
                "    !git clone https://github.com/mouuuuaad/MouaadNet-Ultra.git {REPO_DIR}\n",
                "else:\n",
                "    !cd {REPO_DIR} && git pull\n",
                "\n",
                "os.chdir(REPO_DIR)\n",
                "print(f\"\\n‚úÖ Working: {os.getcwd()}\")\n",
                "print(f\"üìÅ Data: {DATA_DIR}\")\n",
                "print(f\"üíæ Checkpoints: {CKPT_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# CELL 3: Download COCO (Only Once)\n",
                "# ============================================\n",
                "import os\n",
                "\n",
                "def download_if_missing(url, dest):\n",
                "    if not os.path.exists(dest):\n",
                "        name = url.split('/')[-1]\n",
                "        print(f\"üì• Downloading {name}...\")\n",
                "        !wget -q --show-progress {url} -O {dest}\n",
                "    else:\n",
                "        print(f\"‚úÖ Already exists: {dest}\")\n",
                "\n",
                "# Check what exists\n",
                "has_train = os.path.exists(f'{DATA_DIR}/train2017')\n",
                "has_val = os.path.exists(f'{DATA_DIR}/val2017')\n",
                "has_anno = os.path.exists(f'{DATA_DIR}/annotations')\n",
                "\n",
                "if not has_train:\n",
                "    download_if_missing('http://images.cocodataset.org/zips/train2017.zip', f'{DATA_DIR}/train2017.zip')\n",
                "    !cd {DATA_DIR} && unzip -q train2017.zip && rm train2017.zip\n",
                "    \n",
                "if not has_val:\n",
                "    download_if_missing('http://images.cocodataset.org/zips/val2017.zip', f'{DATA_DIR}/val2017.zip')\n",
                "    !cd {DATA_DIR} && unzip -q val2017.zip && rm val2017.zip\n",
                "\n",
                "if not has_anno:\n",
                "    download_if_missing('http://images.cocodataset.org/annotations/annotations_trainval2017.zip', f'{DATA_DIR}/annotations.zip')\n",
                "    !cd {DATA_DIR} && unzip -q annotations.zip && rm annotations.zip\n",
                "\n",
                "print(\"\\n‚úÖ COCO Dataset Ready!\")\n",
                "!ls {DATA_DIR}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# CELL 4: Imports\n",
                "# ============================================\n",
                "import os\n",
                "import sys\n",
                "import numpy as np\n",
                "import cv2\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from torchvision import transforms\n",
                "from PIL import Image\n",
                "from pycocotools.coco import COCO\n",
                "from tqdm.auto import tqdm\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Add repo to path\n",
                "sys.path.insert(0, REPO_DIR)\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"üñ•Ô∏è Device: {device}\")\n",
                "print(f\"üî• PyTorch: {torch.__version__}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# CELL 5: Dataset Class\n",
                "# ============================================\n",
                "\n",
                "def gaussian2d(shape, sigma):\n",
                "    \"\"\"Create 2D Gaussian kernel.\"\"\"\n",
                "    m, n = [(s - 1) / 2 for s in shape]\n",
                "    y, x = np.ogrid[-m:m+1, -n:n+1]\n",
                "    g = np.exp(-(x*x + y*y) / (2*sigma*sigma))\n",
                "    g[g < 1e-7] = 0\n",
                "    return g\n",
                "\n",
                "\n",
                "def draw_gaussian(heatmap, cx, cy, radius):\n",
                "    \"\"\"Draw Gaussian on heatmap at (cx, cy).\"\"\"\n",
                "    diameter = 2 * radius + 1\n",
                "    gaussian = gaussian2d((diameter, diameter), sigma=diameter / 6)\n",
                "    \n",
                "    h, w = heatmap.shape\n",
                "    left = min(cx, radius)\n",
                "    right = min(w - cx, radius + 1)\n",
                "    top = min(cy, radius)\n",
                "    bottom = min(h - cy, radius + 1)\n",
                "    \n",
                "    hm_region = heatmap[cy-top:cy+bottom, cx-left:cx+right]\n",
                "    g_region = gaussian[radius-top:radius+bottom, radius-left:radius+right]\n",
                "    \n",
                "    if hm_region.size > 0 and g_region.size > 0:\n",
                "        np.maximum(hm_region, g_region, out=hm_region)\n",
                "\n",
                "\n",
                "class COCOPersonDataset(Dataset):\n",
                "    \"\"\"COCO Person Detection Dataset.\"\"\"\n",
                "    \n",
                "    def __init__(self, data_dir, split='train', img_size=416, stride=4):\n",
                "        self.data_dir = data_dir\n",
                "        self.split = split\n",
                "        self.img_size = img_size\n",
                "        self.stride = stride\n",
                "        self.out_size = img_size // stride\n",
                "        \n",
                "        # Load COCO\n",
                "        anno_path = f'{data_dir}/annotations/instances_{split}2017.json'\n",
                "        self.coco = COCO(anno_path)\n",
                "        \n",
                "        # Get images with persons (category_id=1)\n",
                "        self.img_ids = self.coco.getImgIds(catIds=[1])\n",
                "        print(f\"‚úÖ {split}: {len(self.img_ids)} images\")\n",
                "        \n",
                "        self.normalize = transforms.Normalize(\n",
                "            mean=[0.485, 0.456, 0.406],\n",
                "            std=[0.229, 0.224, 0.225]\n",
                "        )\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.img_ids)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        # Load image\n",
                "        img_id = self.img_ids[idx]\n",
                "        img_info = self.coco.loadImgs(img_id)[0]\n",
                "        img_path = f\"{self.data_dir}/{self.split}2017/{img_info['file_name']}\"\n",
                "        \n",
                "        img = cv2.imread(img_path)\n",
                "        if img is None:\n",
                "            return self.__getitem__((idx + 1) % len(self))\n",
                "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
                "        h0, w0 = img.shape[:2]\n",
                "        \n",
                "        # Resize maintaining aspect ratio\n",
                "        scale = min(self.img_size / h0, self.img_size / w0)\n",
                "        h1, w1 = int(h0 * scale), int(w0 * scale)\n",
                "        img = cv2.resize(img, (w1, h1))\n",
                "        \n",
                "        # Pad to square\n",
                "        pad_h = self.img_size - h1\n",
                "        pad_w = self.img_size - w1\n",
                "        pad_top = pad_h // 2\n",
                "        pad_left = pad_w // 2\n",
                "        \n",
                "        canvas = np.full((self.img_size, self.img_size, 3), 114, dtype=np.uint8)\n",
                "        canvas[pad_top:pad_top+h1, pad_left:pad_left+w1] = img\n",
                "        \n",
                "        # To tensor\n",
                "        img_t = torch.from_numpy(canvas).permute(2, 0, 1).float() / 255.0\n",
                "        img_t = self.normalize(img_t)\n",
                "        \n",
                "        # Get person annotations\n",
                "        ann_ids = self.coco.getAnnIds(imgIds=img_id, catIds=[1], iscrowd=False)\n",
                "        anns = self.coco.loadAnns(ann_ids)\n",
                "        \n",
                "        # Create targets\n",
                "        heatmap = np.zeros((self.out_size, self.out_size), dtype=np.float32)\n",
                "        wh_map = np.zeros((2, self.out_size, self.out_size), dtype=np.float32)\n",
                "        reg_mask = np.zeros((self.out_size, self.out_size), dtype=np.float32)\n",
                "        \n",
                "        for ann in anns:\n",
                "            x, y, w, h = ann['bbox']\n",
                "            if w < 1 or h < 1:\n",
                "                continue\n",
                "            \n",
                "            # Scale to input size\n",
                "            x = x * scale + pad_left\n",
                "            y = y * scale + pad_top\n",
                "            w = w * scale\n",
                "            h = h * scale\n",
                "            \n",
                "            # Center in output space\n",
                "            cx = (x + w / 2) / self.stride\n",
                "            cy = (y + h / 2) / self.stride\n",
                "            \n",
                "            if 0 <= cx < self.out_size and 0 <= cy < self.out_size:\n",
                "                cx_int = int(cx)\n",
                "                cy_int = int(cy)\n",
                "                \n",
                "                # Radius based on object size\n",
                "                radius = max(1, int(min(w, h) / self.stride / 3))\n",
                "                draw_gaussian(heatmap, cx_int, cy_int, radius)\n",
                "                \n",
                "                # Width/height (normalized)\n",
                "                wh_map[0, cy_int, cx_int] = w / self.img_size\n",
                "                wh_map[1, cy_int, cx_int] = h / self.img_size\n",
                "                reg_mask[cy_int, cx_int] = 1\n",
                "        \n",
                "        return {\n",
                "            'image': img_t,\n",
                "            'heatmap': torch.from_numpy(heatmap[None]),\n",
                "            'wh': torch.from_numpy(wh_map),\n",
                "            'mask': torch.from_numpy(reg_mask),\n",
                "        }\n",
                "\n",
                "print(\"‚úÖ Dataset class defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# CELL 6: Create DataLoaders\n",
                "# ============================================\n",
                "BATCH_SIZE = 16\n",
                "IMG_SIZE = 416\n",
                "\n",
                "train_ds = COCOPersonDataset(DATA_DIR, 'train', IMG_SIZE)\n",
                "val_ds = COCOPersonDataset(DATA_DIR, 'val', IMG_SIZE)\n",
                "\n",
                "train_loader = DataLoader(train_ds, BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
                "val_loader = DataLoader(val_ds, BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
                "\n",
                "print(f\"\\nüìä Train: {len(train_loader)} batches\")\n",
                "print(f\"üìä Val: {len(val_loader)} batches\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# CELL 7: Visualize Data\n",
                "# ============================================\n",
                "batch = next(iter(train_loader))\n",
                "\n",
                "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
                "for i in range(4):\n",
                "    # Image\n",
                "    img = batch['image'][i].permute(1,2,0).numpy()\n",
                "    img = img * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]\n",
                "    img = np.clip(img, 0, 1)\n",
                "    axes[0,i].imshow(img)\n",
                "    axes[0,i].axis('off')\n",
                "    axes[0,i].set_title('Input')\n",
                "    \n",
                "    # Heatmap\n",
                "    hm = batch['heatmap'][i, 0].numpy()\n",
                "    axes[1,i].imshow(hm, cmap='hot')\n",
                "    axes[1,i].axis('off')\n",
                "    axes[1,i].set_title(f'Heatmap (max={hm.max():.2f})')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# CELL 8: Load Model\n",
                "# ============================================\n",
                "from mouaadnet_ultra.model import MouaadNetUltra\n",
                "\n",
                "model = MouaadNetUltra().to(device)\n",
                "print(f\"‚úÖ Model loaded\")\n",
                "print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# CELL 9: Loss Function\n",
                "# ============================================\n",
                "\n",
                "class CenterNetLoss(nn.Module):\n",
                "    \"\"\"CenterNet-style detection loss.\"\"\"\n",
                "    \n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "    \n",
                "    def forward(self, pred_hm, pred_wh, gt_hm, gt_wh, mask):\n",
                "        # Focal Loss for heatmap\n",
                "        pred_hm = torch.clamp(torch.sigmoid(pred_hm), 1e-4, 1 - 1e-4)\n",
                "        \n",
                "        pos_mask = gt_hm.eq(1).float()\n",
                "        neg_mask = gt_hm.lt(1).float()\n",
                "        \n",
                "        pos_loss = -torch.log(pred_hm) * torch.pow(1 - pred_hm, 2) * pos_mask\n",
                "        neg_loss = -torch.log(1 - pred_hm) * torch.pow(pred_hm, 2) * torch.pow(1 - gt_hm, 4) * neg_mask\n",
                "        \n",
                "        num_pos = pos_mask.sum().clamp(min=1)\n",
                "        hm_loss = (pos_loss.sum() + neg_loss.sum()) / num_pos\n",
                "        \n",
                "        # L1 Loss for size regression\n",
                "        mask = mask.unsqueeze(1)\n",
                "        wh_loss = F.l1_loss(pred_wh * mask, gt_wh * mask, reduction='sum')\n",
                "        wh_loss = wh_loss / (mask.sum() + 1e-4)\n",
                "        \n",
                "        total = hm_loss + 0.1 * wh_loss\n",
                "        \n",
                "        return {'total': total, 'hm': hm_loss, 'wh': wh_loss}\n",
                "\n",
                "criterion = CenterNetLoss()\n",
                "print(\"‚úÖ Loss function defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# CELL 10: Training Setup\n",
                "# ============================================\n",
                "EPOCHS = 30\n",
                "LR = 1e-3\n",
                "\n",
                "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
                "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, EPOCHS)\n",
                "scaler = torch.amp.GradScaler('cuda')\n",
                "\n",
                "print(f\"‚úÖ Training setup complete\")\n",
                "print(f\"   Epochs: {EPOCHS}\")\n",
                "print(f\"   Learning Rate: {LR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# CELL 11: Training Functions\n",
                "# ============================================\n",
                "\n",
                "def train_one_epoch(model, loader, optimizer, criterion, scaler):\n",
                "    model.train()\n",
                "    total = 0\n",
                "    \n",
                "    pbar = tqdm(loader, desc='Train')\n",
                "    for batch in pbar:\n",
                "        imgs = batch['image'].to(device)\n",
                "        gt_hm = batch['heatmap'].to(device)\n",
                "        gt_wh = batch['wh'].to(device)\n",
                "        mask = batch['mask'].to(device)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        \n",
                "        with torch.amp.autocast('cuda'):\n",
                "            out = model(imgs)\n",
                "            pred_hm = out['heatmaps'][0]\n",
                "            pred_wh = out['sizes'][0]\n",
                "            loss_dict = criterion(pred_hm, pred_wh, gt_hm, gt_wh, mask)\n",
                "            loss = loss_dict['total']\n",
                "        \n",
                "        if torch.isfinite(loss):\n",
                "            scaler.scale(loss).backward()\n",
                "            scaler.unscale_(optimizer)\n",
                "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
                "            scaler.step(optimizer)\n",
                "            scaler.update()\n",
                "            total += loss.item()\n",
                "        \n",
                "        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
                "    \n",
                "    return total / len(loader)\n",
                "\n",
                "\n",
                "@torch.no_grad()\n",
                "def validate(model, loader, criterion):\n",
                "    model.eval()\n",
                "    total = 0\n",
                "    \n",
                "    for batch in tqdm(loader, desc='Val'):\n",
                "        imgs = batch['image'].to(device)\n",
                "        gt_hm = batch['heatmap'].to(device)\n",
                "        gt_wh = batch['wh'].to(device)\n",
                "        mask = batch['mask'].to(device)\n",
                "        \n",
                "        out = model(imgs)\n",
                "        loss_dict = criterion(out['heatmaps'][0], out['sizes'][0], gt_hm, gt_wh, mask)\n",
                "        total += loss_dict['total'].item()\n",
                "    \n",
                "    return total / len(loader)\n",
                "\n",
                "print(\"‚úÖ Training functions ready\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# CELL 12: TRAIN!\n",
                "# ============================================\n",
                "best_loss = float('inf')\n",
                "history = {'train': [], 'val': []}\n",
                "\n",
                "print(\"=\"*50)\n",
                "print(\"üöÄ Starting Training\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "for epoch in range(EPOCHS):\n",
                "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
                "    \n",
                "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, scaler)\n",
                "    val_loss = validate(model, val_loader, criterion)\n",
                "    scheduler.step()\n",
                "    \n",
                "    history['train'].append(train_loss)\n",
                "    history['val'].append(val_loss)\n",
                "    \n",
                "    print(f\"Train: {train_loss:.4f} | Val: {val_loss:.4f}\")\n",
                "    \n",
                "    # Save\n",
                "    if val_loss < best_loss:\n",
                "        best_loss = val_loss\n",
                "        torch.save({\n",
                "            'epoch': epoch,\n",
                "            'model_state_dict': model.state_dict(),\n",
                "            'loss': best_loss\n",
                "        }, f'{CKPT_DIR}/best.pt')\n",
                "        print(\"‚≠ê Saved best!\")\n",
                "\n",
                "print(f\"\\n‚úÖ Done! Best loss: {best_loss:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# CELL 13: Plot Results\n",
                "# ============================================\n",
                "plt.figure(figsize=(10, 4))\n",
                "plt.plot(history['train'], label='Train')\n",
                "plt.plot(history['val'], label='Val')\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Loss')\n",
                "plt.legend()\n",
                "plt.title('Training Curves')\n",
                "plt.grid(True)\n",
                "plt.savefig(f'{CKPT_DIR}/curves.png')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# CELL 14: Test Detection\n",
                "# ============================================\n",
                "model.eval()\n",
                "batch = next(iter(val_loader))\n",
                "\n",
                "with torch.no_grad():\n",
                "    out = model(batch['image'][:4].to(device))\n",
                "\n",
                "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
                "for i in range(4):\n",
                "    img = batch['image'][i].permute(1,2,0).numpy()\n",
                "    img = img * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]\n",
                "    axes[0,i].imshow(np.clip(img, 0, 1))\n",
                "    axes[0,i].axis('off')\n",
                "    \n",
                "    hm = torch.sigmoid(out['heatmaps'][0][i,0]).cpu().numpy()\n",
                "    axes[1,i].imshow(hm, cmap='hot')\n",
                "    axes[1,i].set_title(f'max={hm.max():.2f}')\n",
                "    axes[1,i].axis('off')\n",
                "\n",
                "plt.suptitle('Detection Results')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# CELL 15: Export\n",
                "# ============================================\n",
                "ckpt = torch.load(f'{CKPT_DIR}/best.pt')\n",
                "model.load_state_dict(ckpt['model_state_dict'])\n",
                "model.eval()\n",
                "model.cpu()\n",
                "\n",
                "# ONNX\n",
                "torch.onnx.export(\n",
                "    model, \n",
                "    torch.randn(1, 3, 416, 416),\n",
                "    f'{CKPT_DIR}/detection.onnx',\n",
                "    input_names=['image'],\n",
                "    opset_version=11\n",
                ")\n",
                "\n",
                "print(f\"‚úÖ Exported!\")\n",
                "print(f\"   PyTorch: {CKPT_DIR}/best.pt\")\n",
                "print(f\"   ONNX: {CKPT_DIR}/detection.onnx\")\n",
                "print(f\"\\nüì• Download and test with:\")\n",
                "print(f\"   python examples/webcam_demo.py --weights best.pt\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# CELL 16: Download (Colab Only)\n",
                "# ============================================\n",
                "try:\n",
                "    from google.colab import files\n",
                "    files.download(f'{CKPT_DIR}/best.pt')\n",
                "    files.download(f'{CKPT_DIR}/detection.onnx')\n",
                "    print(\"üéâ Downloaded!\")\n",
                "except:\n",
                "    print(f\"üìÅ Files saved to: {CKPT_DIR}\")\n",
                "    print(\"   Copy them manually or use Lightning AI file browser\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        },
        "accelerator": "GPU"
    },
    "nbformat": 4,
    "nbformat_minor": 4
}