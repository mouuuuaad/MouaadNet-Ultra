{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸš€ MOUAADNET-ULTRA Training\n",
                "## Human Detection with PA-100k Dataset\n",
                "\n",
                "**Lead Architect:** MOUAAD IDOUFKIR\n",
                "\n",
                "This notebook trains MOUAADNET-ULTRA for human detection using the PA-100k pedestrian attribute dataset.\n",
                "\n",
                "---\n",
                "\n",
                "### ðŸ“‹ Steps:\n",
                "1. Setup environment\n",
                "2. Download PA-100k dataset\n",
                "3. Prepare data loaders\n",
                "4. Train model\n",
                "5. Export to ONNX"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1ï¸âƒ£ Environment Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check GPU\n",
                "!nvidia-smi"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clone MOUAADNET-ULTRA repository\n",
                "!git clone https://github.com/mouaadidoufkir/mouaadnet-ultra.git\n",
                "%cd mouaadnet-ultra\n",
                "\n",
                "# Or upload your local files\n",
                "# from google.colab import files\n",
                "# files.upload()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install -q torch torchvision tqdm scipy"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2ï¸âƒ£ Download PA-100k Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Option 1: Download from Google Drive (if you have it uploaded)\n",
                "# from google.colab import drive\n",
                "# drive.mount('/content/drive')\n",
                "# !cp -r /content/drive/MyDrive/PA-100k ./data/\n",
                "\n",
                "# Option 2: Download from source\n",
                "# PA-100k: https://github.com/xh-liu/HydraPlus-Net#pa-100k-dataset\n",
                "# You need to request access from the authors\n",
                "\n",
                "# Create data directory\n",
                "!mkdir -p data/PA-100k/images\n",
                "\n",
                "print(\"ðŸ“ Please upload PA-100k dataset to data/PA-100k/\")\n",
                "print(\"   Expected structure:\")\n",
                "print(\"   data/PA-100k/\")\n",
                "print(\"   â”œâ”€â”€ images/\")\n",
                "print(\"   â”‚   â”œâ”€â”€ 000001.jpg\")\n",
                "print(\"   â”‚   â”œâ”€â”€ 000002.jpg\")\n",
                "print(\"   â”‚   â””â”€â”€ ...\")\n",
                "print(\"   â””â”€â”€ annotation.mat\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Upload dataset (alternative method)\n",
                "from google.colab import files\n",
                "\n",
                "print(\"Upload PA-100k.zip file:\")\n",
                "uploaded = files.upload()\n",
                "\n",
                "# Unzip\n",
                "!unzip -q PA-100k.zip -d data/\n",
                "!ls data/PA-100k/"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3ï¸âƒ£ Dataset & DataLoader"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from torchvision import transforms\n",
                "from PIL import Image\n",
                "from scipy.io import loadmat\n",
                "from tqdm import tqdm\n",
                "\n",
                "class PA100kDataset(Dataset):\n",
                "    \"\"\"\n",
                "    PA-100k Dataset for Human Detection Training.\n",
                "    \n",
                "    Since PA-100k contains cropped pedestrians, we use it to:\n",
                "    1. Train a person classifier (is this a person?)\n",
                "    2. Extract gender attribute (Male/Female)\n",
                "    \n",
                "    For detection, we generate synthetic heatmaps centered on the image.\n",
                "    \"\"\"\n",
                "    \n",
                "    # PA-100k attribute indices\n",
                "    # Gender is typically attributes 0-1: Female, Male\n",
                "    GENDER_FEMALE_IDX = 0\n",
                "    GENDER_MALE_IDX = 1\n",
                "    \n",
                "    def __init__(self, root_dir, split='train', img_size=416, transform=None):\n",
                "        self.root_dir = root_dir\n",
                "        self.img_size = img_size\n",
                "        self.transform = transform or self._default_transform()\n",
                "        \n",
                "        # Load annotations\n",
                "        anno_path = os.path.join(root_dir, 'annotation.mat')\n",
                "        if os.path.exists(anno_path):\n",
                "            anno = loadmat(anno_path)\n",
                "            \n",
                "            # Get file names and labels\n",
                "            if split == 'train':\n",
                "                self.images = [str(x[0][0]) for x in anno['train_images_name']]\n",
                "                self.labels = anno['train_label']\n",
                "            elif split == 'val':\n",
                "                self.images = [str(x[0][0]) for x in anno['val_images_name']]\n",
                "                self.labels = anno['val_label']\n",
                "            else:  # test\n",
                "                self.images = [str(x[0][0]) for x in anno['test_images_name']]\n",
                "                self.labels = anno['test_label']\n",
                "        else:\n",
                "            # Fallback: load all images from directory\n",
                "            img_dir = os.path.join(root_dir, 'images')\n",
                "            self.images = sorted(os.listdir(img_dir))\n",
                "            self.labels = None\n",
                "        \n",
                "        print(f\"Loaded {len(self.images)} images for {split} split\")\n",
                "    \n",
                "    def _default_transform(self):\n",
                "        return transforms.Compose([\n",
                "            transforms.Resize((self.img_size, self.img_size)),\n",
                "            transforms.ToTensor(),\n",
                "            transforms.Normalize(\n",
                "                mean=[0.485, 0.456, 0.406],\n",
                "                std=[0.229, 0.224, 0.225]\n",
                "            ),\n",
                "        ])\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.images)\n",
                "    \n",
                "    def _generate_heatmap(self, h, w):\n",
                "        \"\"\"Generate Gaussian heatmap centered on image.\"\"\"\n",
                "        heatmap = np.zeros((h, w), dtype=np.float32)\n",
                "        \n",
                "        # Center of the image (person is centered in PA-100k crops)\n",
                "        cx, cy = w // 2, h // 2\n",
                "        \n",
                "        # Create Gaussian\n",
                "        sigma = min(h, w) // 8\n",
                "        x = np.arange(w)\n",
                "        y = np.arange(h)\n",
                "        xx, yy = np.meshgrid(x, y)\n",
                "        \n",
                "        heatmap = np.exp(-((xx - cx)**2 + (yy - cy)**2) / (2 * sigma**2))\n",
                "        \n",
                "        return heatmap\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        # Load image\n",
                "        img_name = self.images[idx]\n",
                "        img_path = os.path.join(self.root_dir, 'images', img_name)\n",
                "        \n",
                "        image = Image.open(img_path).convert('RGB')\n",
                "        \n",
                "        # Apply transforms\n",
                "        if self.transform:\n",
                "            image = self.transform(image)\n",
                "        \n",
                "        # Generate targets\n",
                "        hm_size = self.img_size // 4  # Stride 4 for main heatmap\n",
                "        \n",
                "        # Heatmap (person centered)\n",
                "        heatmap = self._generate_heatmap(hm_size, hm_size)\n",
                "        heatmap = torch.from_numpy(heatmap).unsqueeze(0)  # (1, H, W)\n",
                "        \n",
                "        # Size (normalized bbox size - full image for PA-100k)\n",
                "        size = torch.tensor([0.8, 0.8])  # Person takes ~80% of cropped image\n",
                "        \n",
                "        # Offset (no offset needed - person is centered)\n",
                "        offset = torch.tensor([0.0, 0.0])\n",
                "        \n",
                "        # Gender label\n",
                "        if self.labels is not None:\n",
                "            label = self.labels[idx]\n",
                "            # Female=0, Male=1 in PA-100k\n",
                "            gender = 1.0 if label[self.GENDER_MALE_IDX] == 1 else 0.0\n",
                "        else:\n",
                "            gender = 0.5  # Unknown\n",
                "        \n",
                "        gender = torch.tensor([gender], dtype=torch.float32)\n",
                "        \n",
                "        return {\n",
                "            'image': image,\n",
                "            'heatmap': heatmap,\n",
                "            'size': size,\n",
                "            'offset': offset,\n",
                "            'gender': gender,\n",
                "        }\n",
                "\n",
                "\n",
                "def create_dataloaders(data_dir, batch_size=32, img_size=416, num_workers=4):\n",
                "    \"\"\"Create train and validation dataloaders.\"\"\"\n",
                "    \n",
                "    # Training transforms with augmentation\n",
                "    train_transform = transforms.Compose([\n",
                "        transforms.Resize((img_size + 32, img_size + 32)),\n",
                "        transforms.RandomCrop(img_size),\n",
                "        transforms.RandomHorizontalFlip(p=0.5),\n",
                "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
                "        transforms.ToTensor(),\n",
                "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
                "    ])\n",
                "    \n",
                "    # Validation transforms\n",
                "    val_transform = transforms.Compose([\n",
                "        transforms.Resize((img_size, img_size)),\n",
                "        transforms.ToTensor(),\n",
                "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
                "    ])\n",
                "    \n",
                "    # Create datasets\n",
                "    train_dataset = PA100kDataset(data_dir, split='train', img_size=img_size, transform=train_transform)\n",
                "    val_dataset = PA100kDataset(data_dir, split='val', img_size=img_size, transform=val_transform)\n",
                "    \n",
                "    # Create dataloaders\n",
                "    train_loader = DataLoader(\n",
                "        train_dataset,\n",
                "        batch_size=batch_size,\n",
                "        shuffle=True,\n",
                "        num_workers=num_workers,\n",
                "        pin_memory=True,\n",
                "    )\n",
                "    \n",
                "    val_loader = DataLoader(\n",
                "        val_dataset,\n",
                "        batch_size=batch_size,\n",
                "        shuffle=False,\n",
                "        num_workers=num_workers,\n",
                "        pin_memory=True,\n",
                "    )\n",
                "    \n",
                "    return train_loader, val_loader\n",
                "\n",
                "print(\"âœ“ Dataset classes defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create dataloaders\n",
                "DATA_DIR = 'data/PA-100k'\n",
                "BATCH_SIZE = 32\n",
                "IMG_SIZE = 416\n",
                "\n",
                "train_loader, val_loader = create_dataloaders(\n",
                "    DATA_DIR,\n",
                "    batch_size=BATCH_SIZE,\n",
                "    img_size=IMG_SIZE,\n",
                "    num_workers=2\n",
                ")\n",
                "\n",
                "print(f\"Train batches: {len(train_loader)}\")\n",
                "print(f\"Val batches: {len(val_loader)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize sample\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "batch = next(iter(train_loader))\n",
                "\n",
                "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
                "\n",
                "for i in range(4):\n",
                "    # Image\n",
                "    img = batch['image'][i].permute(1, 2, 0).numpy()\n",
                "    img = img * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]  # Denormalize\n",
                "    img = np.clip(img, 0, 1)\n",
                "    \n",
                "    axes[0, i].imshow(img)\n",
                "    gender = 'Male' if batch['gender'][i] > 0.5 else 'Female'\n",
                "    axes[0, i].set_title(f'Gender: {gender}')\n",
                "    axes[0, i].axis('off')\n",
                "    \n",
                "    # Heatmap\n",
                "    hm = batch['heatmap'][i, 0].numpy()\n",
                "    axes[1, i].imshow(hm, cmap='hot')\n",
                "    axes[1, i].set_title('Heatmap')\n",
                "    axes[1, i].axis('off')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4ï¸âƒ£ Model & Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.insert(0, '.')\n",
                "\n",
                "from mouaadnet_ultra.model import MouaadNetUltra\n",
                "from mouaadnet_ultra.losses import MultiTaskLoss\n",
                "\n",
                "# Create model\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Device: {device}\")\n",
                "\n",
                "model = MouaadNetUltra()\n",
                "model = model.to(device)\n",
                "\n",
                "print(f\"Parameters: {model.count_parameters():,}\")\n",
                "print(f\"Model size: {model.get_model_size_mb():.2f} MB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training configuration\n",
                "EPOCHS = 50\n",
                "LR = 1e-3\n",
                "WEIGHT_DECAY = 1e-4\n",
                "\n",
                "# Optimizer\n",
                "optimizer = torch.optim.AdamW(\n",
                "    model.parameters(),\n",
                "    lr=LR,\n",
                "    weight_decay=WEIGHT_DECAY\n",
                ")\n",
                "\n",
                "# Scheduler (One-Cycle)\n",
                "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
                "    optimizer,\n",
                "    max_lr=LR * 10,\n",
                "    epochs=EPOCHS,\n",
                "    steps_per_epoch=len(train_loader),\n",
                "    pct_start=0.3,\n",
                ")\n",
                "\n",
                "# Loss\n",
                "criterion = MultiTaskLoss(\n",
                "    det_weight=1.0,\n",
                "    gender_weight=1.0,\n",
                "    gender_pos_weight=5.0,\n",
                ")\n",
                "\n",
                "# Mixed precision\n",
                "scaler = torch.cuda.amp.GradScaler()\n",
                "\n",
                "print(\"âœ“ Training setup complete\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_epoch(model, loader, optimizer, scheduler, criterion, scaler, device):\n",
                "    \"\"\"Train for one epoch.\"\"\"\n",
                "    model.train()\n",
                "    total_loss = 0\n",
                "    \n",
                "    pbar = tqdm(loader, desc='Training')\n",
                "    for batch in pbar:\n",
                "        images = batch['image'].to(device)\n",
                "        \n",
                "        # Prepare targets\n",
                "        targets = {\n",
                "            'heatmaps': [batch['heatmap'].to(device)],\n",
                "            'sizes': [batch['size'].unsqueeze(-1).unsqueeze(-1).expand(-1, -1, images.shape[2]//4, images.shape[3]//4).to(device)],\n",
                "            'offsets': [batch['offset'].unsqueeze(-1).unsqueeze(-1).expand(-1, -1, images.shape[2]//4, images.shape[3]//4).to(device)],\n",
                "            'gender_labels': batch['gender'].to(device),\n",
                "        }\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        \n",
                "        # Forward with mixed precision\n",
                "        with torch.cuda.amp.autocast():\n",
                "            outputs = model(images)\n",
                "            \n",
                "            # Reshape predictions for loss\n",
                "            predictions = {\n",
                "                'heatmaps': [outputs['heatmaps'][0]],  # Use first scale\n",
                "                'sizes': [outputs['sizes'][0]],\n",
                "                'offsets': [outputs['offsets'][0]],\n",
                "                'gender': outputs['gender'],\n",
                "            }\n",
                "            \n",
                "            losses = criterion(predictions, targets)\n",
                "            loss = losses['total']\n",
                "        \n",
                "        # Backward\n",
                "        scaler.scale(loss).backward()\n",
                "        scaler.step(optimizer)\n",
                "        scaler.update()\n",
                "        scheduler.step()\n",
                "        \n",
                "        total_loss += loss.item()\n",
                "        pbar.set_postfix({'loss': loss.item(), 'lr': scheduler.get_last_lr()[0]})\n",
                "    \n",
                "    return total_loss / len(loader)\n",
                "\n",
                "\n",
                "@torch.no_grad()\n",
                "def validate(model, loader, criterion, device):\n",
                "    \"\"\"Validate model.\"\"\"\n",
                "    model.eval()\n",
                "    total_loss = 0\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    \n",
                "    for batch in tqdm(loader, desc='Validating'):\n",
                "        images = batch['image'].to(device)\n",
                "        \n",
                "        targets = {\n",
                "            'heatmaps': [batch['heatmap'].to(device)],\n",
                "            'sizes': [batch['size'].unsqueeze(-1).unsqueeze(-1).expand(-1, -1, images.shape[2]//4, images.shape[3]//4).to(device)],\n",
                "            'offsets': [batch['offset'].unsqueeze(-1).unsqueeze(-1).expand(-1, -1, images.shape[2]//4, images.shape[3]//4).to(device)],\n",
                "            'gender_labels': batch['gender'].to(device),\n",
                "        }\n",
                "        \n",
                "        with torch.cuda.amp.autocast():\n",
                "            outputs = model(images)\n",
                "            \n",
                "            predictions = {\n",
                "                'heatmaps': [outputs['heatmaps'][0]],\n",
                "                'sizes': [outputs['sizes'][0]],\n",
                "                'offsets': [outputs['offsets'][0]],\n",
                "                'gender': outputs['gender'],\n",
                "            }\n",
                "            \n",
                "            losses = criterion(predictions, targets)\n",
                "            loss = losses['total']\n",
                "        \n",
                "        total_loss += loss.item()\n",
                "        \n",
                "        # Gender accuracy\n",
                "        gender_pred = (torch.sigmoid(outputs['gender']) > 0.5).float()\n",
                "        correct += (gender_pred == batch['gender'].to(device)).sum().item()\n",
                "        total += batch['gender'].size(0)\n",
                "    \n",
                "    accuracy = correct / total * 100\n",
                "    return total_loss / len(loader), accuracy\n",
                "\n",
                "print(\"âœ“ Training functions defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train!\n",
                "best_loss = float('inf')\n",
                "history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"Starting Training\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "for epoch in range(EPOCHS):\n",
                "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
                "    \n",
                "    # Train\n",
                "    train_loss = train_epoch(\n",
                "        model, train_loader, optimizer, scheduler, criterion, scaler, device\n",
                "    )\n",
                "    history['train_loss'].append(train_loss)\n",
                "    \n",
                "    # Validate\n",
                "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
                "    history['val_loss'].append(val_loss)\n",
                "    history['val_acc'].append(val_acc)\n",
                "    \n",
                "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
                "    \n",
                "    # Save best\n",
                "    if val_loss < best_loss:\n",
                "        best_loss = val_loss\n",
                "        torch.save({\n",
                "            'epoch': epoch,\n",
                "            'model_state_dict': model.state_dict(),\n",
                "            'optimizer_state_dict': optimizer.state_dict(),\n",
                "            'best_loss': best_loss,\n",
                "            'config': model.config,\n",
                "        }, 'best_model.pt')\n",
                "        print(\"  â†’ Saved best model!\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"Training Complete!\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot training history\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "ax1.plot(history['train_loss'], label='Train')\n",
                "ax1.plot(history['val_loss'], label='Val')\n",
                "ax1.set_xlabel('Epoch')\n",
                "ax1.set_ylabel('Loss')\n",
                "ax1.set_title('Training Loss')\n",
                "ax1.legend()\n",
                "ax1.grid(True)\n",
                "\n",
                "ax2.plot(history['val_acc'])\n",
                "ax2.set_xlabel('Epoch')\n",
                "ax2.set_ylabel('Accuracy (%)')\n",
                "ax2.set_title('Gender Classification Accuracy')\n",
                "ax2.grid(True)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('training_history.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5ï¸âƒ£ Export Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load best model\n",
                "checkpoint = torch.load('best_model.pt')\n",
                "model.load_state_dict(checkpoint['model_state_dict'])\n",
                "model.eval()\n",
                "\n",
                "print(f\"Loaded best model from epoch {checkpoint['epoch']}\")\n",
                "print(f\"Best validation loss: {checkpoint['best_loss']:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Fuse for inference\n",
                "model.fuse_for_inference()\n",
                "print(\"Model fused for inference\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Export to ONNX\n",
                "dummy_input = torch.randn(1, 3, 416, 416).to(device)\n",
                "\n",
                "torch.onnx.export(\n",
                "    model,\n",
                "    dummy_input,\n",
                "    'mouaadnet_ultra_pa100k.onnx',\n",
                "    input_names=['input'],\n",
                "    output_names=['heatmap', 'size', 'offset', 'gender'],\n",
                "    dynamic_axes={'input': {0: 'batch'}},\n",
                "    opset_version=12,\n",
                ")\n",
                "\n",
                "print(\"âœ“ Exported to ONNX: mouaadnet_ultra_pa100k.onnx\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Download trained files\n",
                "from google.colab import files\n",
                "\n",
                "files.download('best_model.pt')\n",
                "files.download('mouaadnet_ultra_pa100k.onnx')\n",
                "files.download('training_history.png')\n",
                "\n",
                "print(\"\\nðŸŽ‰ Training complete! Download your files above.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ“Š Final Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*60)\n",
                "print(\"ðŸŽ‰ MOUAADNET-ULTRA Training Complete!\")\n",
                "print(\"=\"*60)\n",
                "print(f\"\\nModel: MOUAADNET-ULTRA\")\n",
                "print(f\"Dataset: PA-100k\")\n",
                "print(f\"Parameters: {model.count_parameters():,}\")\n",
                "print(f\"Best Val Loss: {best_loss:.4f}\")\n",
                "print(f\"Final Val Accuracy: {history['val_acc'][-1]:.2f}%\")\n",
                "print(f\"\\nExported files:\")\n",
                "print(f\"  - best_model.pt (PyTorch)\")\n",
                "print(f\"  - mouaadnet_ultra_pa100k.onnx (ONNX)\")\n",
                "print(\"=\"*60)"
            ]
        }
    ]
}