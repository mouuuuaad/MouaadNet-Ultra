{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üéØ MOUAADNET-ULTRA Detection Training\n",
                "## Production-Grade AI for Human Detection\n",
                "\n",
                "**Author:** MOUAAD IDOUFKIR\n",
                "\n",
                "### ‚ö° Features:\n",
                "- **Speed**: Mixed precision (FP16), efficient data loading\n",
                "- **Memory**: Gradient checkpointing, optimized batch processing\n",
                "- **Robust**: Full error handling, checkpoint recovery\n",
                "- **Export**: ONNX with dynamic axes\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title 1Ô∏è‚É£ Setup Environment\n",
                "!nvidia-smi\n",
                "!pip install -q torch torchvision tqdm pycocotools opencv-python onnx"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title 2Ô∏è‚É£ Configuration\n",
                "import os\n",
                "\n",
                "# Auto-detect platform\n",
                "if os.path.exists('/teamspace'):\n",
                "    WORK = '/teamspace/studios/this_studio'\n",
                "else:\n",
                "    WORK = '/content'\n",
                "\n",
                "REPO = f'{WORK}/MouaadNet-Ultra'\n",
                "DATA = f'{WORK}/coco'\n",
                "CKPT = f'{WORK}/checkpoints'\n",
                "\n",
                "os.makedirs(DATA, exist_ok=True)\n",
                "os.makedirs(CKPT, exist_ok=True)\n",
                "\n",
                "print(f'üìÅ Work: {WORK}')\n",
                "print(f'üìÅ Data: {DATA}')\n",
                "print(f'üìÅ Checkpoints: {CKPT}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title 3Ô∏è‚É£ Clone Repository\n",
                "import os\n",
                "\n",
                "if not os.path.exists(REPO):\n",
                "    !git clone https://github.com/mouuuuaad/MouaadNet-Ultra.git {REPO}\n",
                "else:\n",
                "    !cd {REPO} && git pull\n",
                "\n",
                "os.chdir(REPO)\n",
                "print(f'‚úÖ Repository ready: {os.getcwd()}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title 4Ô∏è‚É£ Download COCO Dataset\n",
                "import os\n",
                "\n",
                "def download(url, path):\n",
                "    if not os.path.exists(path):\n",
                "        print(f'üì• Downloading {url.split(\"/\")[-1]}...')\n",
                "        !wget -q --show-progress {url} -O {path}.zip\n",
                "        !cd {DATA} && unzip -q {path}.zip && rm {path}.zip\n",
                "    else:\n",
                "        print(f'‚úÖ Exists: {path}')\n",
                "\n",
                "download('http://images.cocodataset.org/zips/train2017.zip', f'{DATA}/train2017')\n",
                "download('http://images.cocodataset.org/zips/val2017.zip', f'{DATA}/val2017')\n",
                "download('http://images.cocodataset.org/annotations/annotations_trainval2017.zip', f'{DATA}/annotations')\n",
                "\n",
                "print('\\n‚úÖ COCO Dataset Ready!')\n",
                "!ls {DATA}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title 5Ô∏è‚É£ Run Training\n",
                "EPOCHS = 50  #@param {type:\"integer\"}\n",
                "BATCH_SIZE = 16  #@param {type:\"integer\"}\n",
                "LR = 0.001  #@param {type:\"number\"}\n",
                "\n",
                "!python training/train_detection.py \\\n",
                "    --data {DATA} \\\n",
                "    --epochs {EPOCHS} \\\n",
                "    --batch-size {BATCH_SIZE} \\\n",
                "    --lr {LR} \\\n",
                "    --save-dir {CKPT} \\\n",
                "    --export"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title 6Ô∏è‚É£ Visualize Results\n",
                "import matplotlib.pyplot as plt\n",
                "import torch\n",
                "\n",
                "# Load history from checkpoint\n",
                "ckpt = torch.load(f'{CKPT}/best.pt', map_location='cpu')\n",
                "history = ckpt.get('history', {})\n",
                "\n",
                "if history:\n",
                "    plt.figure(figsize=(10, 4))\n",
                "    plt.plot(history['train'], label='Train')\n",
                "    plt.plot(history['val'], label='Val')\n",
                "    plt.xlabel('Epoch')\n",
                "    plt.ylabel('Loss')\n",
                "    plt.legend()\n",
                "    plt.title(f'Training (Best: {ckpt[\"best_loss\"]:.4f})')\n",
                "    plt.grid(True)\n",
                "    plt.savefig(f'{CKPT}/curves.png')\n",
                "    plt.show()\n",
                "\n",
                "print(f'\\n‚úÖ Best loss: {ckpt[\"best_loss\"]:.4f}')\n",
                "print(f'üìÅ Files: {CKPT}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title 7Ô∏è‚É£ Test Detection\n",
                "import sys\n",
                "import numpy as np\n",
                "import torch\n",
                "import cv2\n",
                "import matplotlib.pyplot as plt\n",
                "from PIL import Image\n",
                "\n",
                "sys.path.insert(0, REPO)\n",
                "from mouaadnet_ultra.model import MouaadNetUltra\n",
                "\n",
                "# Load model\n",
                "model = MouaadNetUltra()\n",
                "ckpt = torch.load(f'{CKPT}/best.pt', map_location='cpu')\n",
                "model.load_state_dict(ckpt['model_state_dict'])\n",
                "model.eval()\n",
                "\n",
                "# Test on sample image\n",
                "img_path = f'{DATA}/val2017/000000001268.jpg'  # Sample\n",
                "img = cv2.imread(img_path)\n",
                "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
                "img_resized = cv2.resize(img, (416, 416))\n",
                "\n",
                "# Preprocess\n",
                "x = img_resized.astype(np.float32) / 255.0\n",
                "x = (x - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]\n",
                "x = torch.from_numpy(x.transpose(2, 0, 1)).unsqueeze(0).float()\n",
                "\n",
                "# Inference\n",
                "with torch.no_grad():\n",
                "    out = model(x)\n",
                "    hm = torch.sigmoid(out['heatmaps'][0][0, 0]).numpy()\n",
                "\n",
                "# Show\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
                "ax1.imshow(img_resized)\n",
                "ax1.set_title('Input')\n",
                "ax1.axis('off')\n",
                "ax2.imshow(hm, cmap='hot')\n",
                "ax2.set_title(f'Heatmap (max={hm.max():.2f})')\n",
                "ax2.axis('off')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title 8Ô∏è‚É£ Download Models\n",
                "import os\n",
                "\n",
                "print('üì¶ Available models:')\n",
                "for f in os.listdir(CKPT):\n",
                "    if f.endswith('.pt') or f.endswith('.onnx'):\n",
                "        size = os.path.getsize(f'{CKPT}/{f}') / 1e6\n",
                "        print(f'   {f} ({size:.1f} MB)')\n",
                "\n",
                "# Download (Colab only)\n",
                "try:\n",
                "    from google.colab import files\n",
                "    files.download(f'{CKPT}/best.pt')\n",
                "    files.download(f'{CKPT}/detection.onnx')\n",
                "except:\n",
                "    print(f'\\nüìÅ Copy from: {CKPT}')\n",
                "\n",
                "print('\\nüéâ Use with webcam:')\n",
                "print('python examples/webcam_demo.py --weights best.pt')"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}