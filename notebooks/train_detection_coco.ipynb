{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üéØ MOUAADNET-ULTRA: Human Detection Training\n",
                "## Lightning AI Studio + COCO 2017\n",
                "\n",
                "**Lead Architect:** MOUAAD IDOUFKIR\n",
                "\n",
                "### ‚ö° Lightning AI Benefits:\n",
                "- Persistent storage (no re-download)\n",
                "- Better GPUs (A10G, L4, etc.)\n",
                "- Longer runtime\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import subprocess\n",
                "import sys\n",
                "\n",
                "# Check GPU\n",
                "!nvidia-smi\n",
                "\n",
                "# Install dependencies\n",
                "!pip install -q torch torchvision tqdm pycocotools"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clone repo (or use Lightning AI's Git integration)\n",
                "import os\n",
                "\n",
                "REPO_DIR = '/teamspace/studios/this_studio/MouaadNet-Ultra'\n",
                "\n",
                "if not os.path.exists(REPO_DIR):\n",
                "    !git clone https://github.com/mouuuuaad/MouaadNet-Ultra.git {REPO_DIR}\n",
                "else:\n",
                "    print(f\"‚úÖ Repo already exists at {REPO_DIR}\")\n",
                "    !cd {REPO_DIR} && git pull\n",
                "\n",
                "os.chdir(REPO_DIR)\n",
                "print(f\"Working directory: {os.getcwd()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Download COCO 2017 (Persistent Storage)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Lightning AI has persistent storage - data stays between sessions!\n",
                "DATA_DIR = '/teamspace/studios/this_studio/data/coco'\n",
                "\n",
                "import os\n",
                "os.makedirs(DATA_DIR, exist_ok=True)\n",
                "\n",
                "# Check if already downloaded\n",
                "train_exists = os.path.exists(f'{DATA_DIR}/train2017')\n",
                "val_exists = os.path.exists(f'{DATA_DIR}/val2017')\n",
                "anno_exists = os.path.exists(f'{DATA_DIR}/annotations')\n",
                "\n",
                "print(f\"Train images: {'‚úÖ exists' if train_exists else '‚ùå missing'}\")\n",
                "print(f\"Val images: {'‚úÖ exists' if val_exists else '‚ùå missing'}\")\n",
                "print(f\"Annotations: {'‚úÖ exists' if anno_exists else '‚ùå missing'}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Download only if needed (persistent storage saves time!)\n",
                "if not train_exists:\n",
                "    print(\"üì• Downloading train2017 (~18GB)...\")\n",
                "    !wget -q --show-progress http://images.cocodataset.org/zips/train2017.zip -O {DATA_DIR}/train2017.zip\n",
                "    !cd {DATA_DIR} && unzip -q train2017.zip && rm train2017.zip\n",
                "\n",
                "if not val_exists:\n",
                "    print(\"üì• Downloading val2017 (~1GB)...\")\n",
                "    !wget -q --show-progress http://images.cocodataset.org/zips/val2017.zip -O {DATA_DIR}/val2017.zip\n",
                "    !cd {DATA_DIR} && unzip -q val2017.zip && rm val2017.zip\n",
                "\n",
                "if not anno_exists:\n",
                "    print(\"üì• Downloading annotations...\")\n",
                "    !wget -q --show-progress http://images.cocodataset.org/annotations/annotations_trainval2017.zip -O {DATA_DIR}/annotations.zip\n",
                "    !cd {DATA_DIR} && unzip -q annotations.zip && rm annotations.zip\n",
                "\n",
                "print(\"\\n‚úÖ COCO 2017 ready!\")\n",
                "!ls -la {DATA_DIR}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from torchvision import transforms\n",
                "from PIL import Image\n",
                "from pycocotools.coco import COCO\n",
                "from tqdm import tqdm\n",
                "import cv2\n",
                "\n",
                "\n",
                "def gaussian2D(shape, sigma=1):\n",
                "    m, n = [(ss - 1.) / 2. for ss in shape]\n",
                "    y, x = np.ogrid[-m:m+1, -n:n+1]\n",
                "    h = np.exp(-(x*x + y*y) / (2*sigma*sigma))\n",
                "    h[h < np.finfo(h.dtype).eps * h.max()] = 0\n",
                "    return h\n",
                "\n",
                "\n",
                "def draw_gaussian(heatmap, center, radius, k=1):\n",
                "    diameter = 2 * radius + 1\n",
                "    gaussian = gaussian2D((diameter, diameter), sigma=diameter / 6)\n",
                "    x, y = int(center[0]), int(center[1])\n",
                "    height, width = heatmap.shape[0:2]\n",
                "    left, right = min(x, radius), min(width - x, radius + 1)\n",
                "    top, bottom = min(y, radius), min(height - y, radius + 1)\n",
                "    masked_heatmap = heatmap[y - top:y + bottom, x - left:x + right]\n",
                "    masked_gaussian = gaussian[radius - top:radius + bottom, radius - left:radius + right]\n",
                "    if min(masked_gaussian.shape) > 0 and min(masked_heatmap.shape) > 0:\n",
                "        np.maximum(masked_heatmap, masked_gaussian * k, out=masked_heatmap)\n",
                "    return heatmap\n",
                "\n",
                "\n",
                "def gaussian_radius(det_size, min_overlap=0.7):\n",
                "    height, width = det_size\n",
                "    a1, b1 = 1, (height + width)\n",
                "    c1 = width * height * (1 - min_overlap) / (1 + min_overlap)\n",
                "    sq1 = np.sqrt(b1 ** 2 - 4 * a1 * c1)\n",
                "    return max(0, int((b1 + sq1) / 2))\n",
                "\n",
                "\n",
                "class COCOPersonDataset(Dataset):\n",
                "    PERSON_CAT_ID = 1\n",
                "    \n",
                "    def __init__(self, root_dir, split='train', img_size=416, down_ratio=4):\n",
                "        self.root = root_dir\n",
                "        self.split = split\n",
                "        self.img_size = img_size\n",
                "        self.down_ratio = down_ratio\n",
                "        self.output_size = img_size // down_ratio\n",
                "        \n",
                "        anno_file = os.path.join(root_dir, 'annotations', f'instances_{split}2017.json')\n",
                "        self.coco = COCO(anno_file)\n",
                "        self.img_ids = self.coco.getImgIds(catIds=[self.PERSON_CAT_ID])\n",
                "        print(f\"‚úÖ {split}: {len(self.img_ids)} images with persons\")\n",
                "        \n",
                "        self.transform = transforms.Compose([\n",
                "            transforms.ToTensor(),\n",
                "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
                "        ])\n",
                "    \n",
                "    def __len__(self): return len(self.img_ids)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        img_id = self.img_ids[idx]\n",
                "        img_info = self.coco.loadImgs(img_id)[0]\n",
                "        img_path = os.path.join(self.root, f'{self.split}2017', img_info['file_name'])\n",
                "        \n",
                "        img = cv2.imread(img_path)\n",
                "        if img is None:\n",
                "            return self.__getitem__((idx + 1) % len(self))\n",
                "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
                "        orig_h, orig_w = img.shape[:2]\n",
                "        \n",
                "        # Resize\n",
                "        scale = min(self.img_size / orig_h, self.img_size / orig_w)\n",
                "        new_h, new_w = int(orig_h * scale), int(orig_w * scale)\n",
                "        img_resized = cv2.resize(img, (new_w, new_h))\n",
                "        \n",
                "        # Pad\n",
                "        pad_h, pad_w = self.img_size - new_h, self.img_size - new_w\n",
                "        pad_top, pad_left = pad_h // 2, pad_w // 2\n",
                "        img_padded = np.full((self.img_size, self.img_size, 3), 114, dtype=np.uint8)\n",
                "        img_padded[pad_top:pad_top+new_h, pad_left:pad_left+new_w] = img_resized\n",
                "        \n",
                "        # Annotations\n",
                "        ann_ids = self.coco.getAnnIds(imgIds=img_id, catIds=[self.PERSON_CAT_ID], iscrowd=False)\n",
                "        anns = self.coco.loadAnns(ann_ids)\n",
                "        \n",
                "        # Targets\n",
                "        heatmap = np.zeros((self.output_size, self.output_size), dtype=np.float32)\n",
                "        size_map = np.zeros((2, self.output_size, self.output_size), dtype=np.float32)\n",
                "        offset_map = np.zeros((2, self.output_size, self.output_size), dtype=np.float32)\n",
                "        reg_mask = np.zeros((self.output_size, self.output_size), dtype=np.float32)\n",
                "        \n",
                "        for ann in anns:\n",
                "            bbox = ann['bbox']\n",
                "            if bbox[2] < 5 or bbox[3] < 5: continue\n",
                "            \n",
                "            x = bbox[0] * scale + pad_left\n",
                "            y = bbox[1] * scale + pad_top\n",
                "            w, h = bbox[2] * scale, bbox[3] * scale\n",
                "            \n",
                "            cx = np.clip((x + w/2) / self.down_ratio, 0, self.output_size - 1)\n",
                "            cy = np.clip((y + h/2) / self.down_ratio, 0, self.output_size - 1)\n",
                "            cx_int, cy_int = int(cx), int(cy)\n",
                "            \n",
                "            radius = max(1, gaussian_radius((h/self.down_ratio, w/self.down_ratio)))\n",
                "            draw_gaussian(heatmap, (cx_int, cy_int), radius)\n",
                "            \n",
                "            size_map[0, cy_int, cx_int] = w / self.img_size\n",
                "            size_map[1, cy_int, cx_int] = h / self.img_size\n",
                "            offset_map[0, cy_int, cx_int] = cx - cx_int\n",
                "            offset_map[1, cy_int, cx_int] = cy - cy_int\n",
                "            reg_mask[cy_int, cx_int] = 1\n",
                "        \n",
                "        img_tensor = self.transform(Image.fromarray(img_padded))\n",
                "        \n",
                "        return {\n",
                "            'image': img_tensor,\n",
                "            'heatmap': torch.from_numpy(heatmap).unsqueeze(0),\n",
                "            'size': torch.from_numpy(size_map),\n",
                "            'offset': torch.from_numpy(offset_map),\n",
                "            'reg_mask': torch.from_numpy(reg_mask),\n",
                "        }\n",
                "\n",
                "print(\"‚úÖ Dataset class ready\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "BATCH_SIZE = 16\n",
                "IMG_SIZE = 416\n",
                "\n",
                "train_dataset = COCOPersonDataset(DATA_DIR, 'train', IMG_SIZE)\n",
                "val_dataset = COCOPersonDataset(DATA_DIR, 'val', IMG_SIZE)\n",
                "\n",
                "train_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True, num_workers=8, pin_memory=True)\n",
                "val_loader = DataLoader(val_dataset, BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
                "\n",
                "print(f\"\\nTrain: {len(train_loader)} batches | Val: {len(val_loader)} batches\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "batch = next(iter(train_loader))\n",
                "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
                "\n",
                "for i in range(4):\n",
                "    img = batch['image'][i].permute(1, 2, 0).numpy()\n",
                "    img = img * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]\n",
                "    img = np.clip(img, 0, 1)\n",
                "    axes[0, i].imshow(img)\n",
                "    axes[0, i].axis('off')\n",
                "    \n",
                "    hm = batch['heatmap'][i, 0].numpy()\n",
                "    axes[1, i].imshow(hm, cmap='hot')\n",
                "    axes[1, i].axis('off')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4Ô∏è‚É£ Model & Loss"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.insert(0, '.')\n",
                "from mouaadnet_ultra.model import MouaadNetUltra\n",
                "\n",
                "device = torch.device('cuda')\n",
                "model = MouaadNetUltra().to(device)\n",
                "print(f\"Device: {device}\")\n",
                "print(f\"Parameters: {model.count_parameters():,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class DetectionLoss(nn.Module):\n",
                "    def __init__(self, hm_weight=1.0, size_weight=0.1, offset_weight=1.0):\n",
                "        super().__init__()\n",
                "        self.hm_weight = hm_weight\n",
                "        self.size_weight = size_weight\n",
                "        self.offset_weight = offset_weight\n",
                "    \n",
                "    def focal_loss(self, pred, target):\n",
                "        pred = torch.clamp(torch.sigmoid(pred), 1e-6, 1 - 1e-6)\n",
                "        pos_mask = target.eq(1).float()\n",
                "        neg_mask = target.lt(1).float()\n",
                "        pos_loss = -torch.log(pred) * torch.pow(1 - pred, 2) * pos_mask\n",
                "        neg_loss = -torch.log(1 - pred) * torch.pow(pred, 2) * torch.pow(1 - target, 4) * neg_mask\n",
                "        return (pos_loss.sum() + neg_loss.sum()) / pos_mask.sum().clamp(min=1)\n",
                "    \n",
                "    def reg_loss(self, pred, target, mask):\n",
                "        mask = mask.unsqueeze(1).expand_as(pred)\n",
                "        return F.l1_loss(pred * mask, target * mask, reduction='sum') / mask.sum().clamp(min=1)\n",
                "    \n",
                "    def forward(self, pred_hm, pred_size, pred_offset, target_hm, target_size, target_offset, reg_mask):\n",
                "        hm_loss = self.focal_loss(pred_hm, target_hm)\n",
                "        size_loss = self.reg_loss(pred_size, target_size, reg_mask)\n",
                "        offset_loss = self.reg_loss(pred_offset, target_offset, reg_mask)\n",
                "        total = self.hm_weight * hm_loss + self.size_weight * size_loss + self.offset_weight * offset_loss\n",
                "        return {'total': total, 'hm': hm_loss, 'size': size_loss, 'offset': offset_loss}\n",
                "\n",
                "criterion = DetectionLoss()\n",
                "print(\"‚úÖ Loss ready\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "EPOCHS = 50\n",
                "LR = 1e-3\n",
                "\n",
                "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
                "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-6)\n",
                "scaler = torch.amp.GradScaler('cuda')\n",
                "\n",
                "# Checkpoint dir\n",
                "CKPT_DIR = '/teamspace/studios/this_studio/checkpoints'\n",
                "os.makedirs(CKPT_DIR, exist_ok=True)\n",
                "\n",
                "print(f\"‚úÖ Training: {EPOCHS} epochs, checkpoints in {CKPT_DIR}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5Ô∏è‚É£ Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_epoch(model, loader, optimizer, criterion, scaler, device):\n",
                "    model.train()\n",
                "    total_loss, total_hm = 0, 0\n",
                "    \n",
                "    pbar = tqdm(loader, desc='Training')\n",
                "    for batch in pbar:\n",
                "        images = batch['image'].to(device, non_blocking=True)\n",
                "        heatmaps = batch['heatmap'].to(device, non_blocking=True)\n",
                "        sizes = batch['size'].to(device, non_blocking=True)\n",
                "        offsets = batch['offset'].to(device, non_blocking=True)\n",
                "        reg_mask = batch['reg_mask'].to(device, non_blocking=True)\n",
                "        \n",
                "        optimizer.zero_grad(set_to_none=True)\n",
                "        \n",
                "        with torch.amp.autocast('cuda'):\n",
                "            outputs = model(images)\n",
                "            losses = criterion(\n",
                "                outputs['heatmaps'][0], outputs['sizes'][0], outputs['offsets'][0],\n",
                "                heatmaps, sizes, offsets, reg_mask\n",
                "            )\n",
                "            loss = losses['total']\n",
                "        \n",
                "        if not torch.isnan(loss):\n",
                "            scaler.scale(loss).backward()\n",
                "            scaler.unscale_(optimizer)\n",
                "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
                "            scaler.step(optimizer)\n",
                "            scaler.update()\n",
                "            \n",
                "            total_loss += loss.item()\n",
                "            total_hm += losses['hm'].item()\n",
                "        \n",
                "        pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
                "    \n",
                "    n = len(loader)\n",
                "    return total_loss/n, total_hm/n\n",
                "\n",
                "\n",
                "@torch.no_grad()\n",
                "def validate(model, loader, criterion, device):\n",
                "    model.eval()\n",
                "    total_loss = 0\n",
                "    \n",
                "    for batch in tqdm(loader, desc='Validating'):\n",
                "        images = batch['image'].to(device)\n",
                "        heatmaps = batch['heatmap'].to(device)\n",
                "        sizes = batch['size'].to(device)\n",
                "        offsets = batch['offset'].to(device)\n",
                "        reg_mask = batch['reg_mask'].to(device)\n",
                "        \n",
                "        with torch.amp.autocast('cuda'):\n",
                "            outputs = model(images)\n",
                "            losses = criterion(\n",
                "                outputs['heatmaps'][0], outputs['sizes'][0], outputs['offsets'][0],\n",
                "                heatmaps, sizes, offsets, reg_mask\n",
                "            )\n",
                "        total_loss += losses['total'].item()\n",
                "    \n",
                "    return total_loss / len(loader)\n",
                "\n",
                "print(\"‚úÖ Training functions ready\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# üöÄ TRAIN!\n",
                "best_loss = float('inf')\n",
                "history = {'loss': [], 'val_loss': []}\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"üéØ Training MOUAADNET-ULTRA on Lightning AI\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "for epoch in range(EPOCHS):\n",
                "    print(f\"\\nüìç Epoch {epoch+1}/{EPOCHS}\")\n",
                "    \n",
                "    train_loss, hm_loss = train_epoch(model, train_loader, optimizer, criterion, scaler, device)\n",
                "    val_loss = validate(model, val_loader, criterion, device)\n",
                "    scheduler.step()\n",
                "    \n",
                "    history['loss'].append(train_loss)\n",
                "    history['val_loss'].append(val_loss)\n",
                "    \n",
                "    print(f\"   Train: {train_loss:.4f} | Val: {val_loss:.4f}\")\n",
                "    \n",
                "    # Save checkpoints\n",
                "    torch.save({\n",
                "        'epoch': epoch,\n",
                "        'model_state_dict': model.state_dict(),\n",
                "        'optimizer_state_dict': optimizer.state_dict(),\n",
                "        'val_loss': val_loss,\n",
                "    }, f'{CKPT_DIR}/epoch_{epoch+1}.pt')\n",
                "    \n",
                "    if val_loss < best_loss:\n",
                "        best_loss = val_loss\n",
                "        torch.save({\n",
                "            'epoch': epoch,\n",
                "            'model_state_dict': model.state_dict(),\n",
                "            'best_loss': best_loss,\n",
                "        }, f'{CKPT_DIR}/best_detection.pt')\n",
                "        print(\"   ‚≠ê Best model!\")\n",
                "\n",
                "print(f\"\\n‚úÖ Done! Best loss: {best_loss:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "plt.figure(figsize=(10, 4))\n",
                "plt.plot(history['loss'], label='Train')\n",
                "plt.plot(history['val_loss'], label='Val')\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Loss')\n",
                "plt.legend()\n",
                "plt.title('Detection Training')\n",
                "plt.grid(True)\n",
                "plt.savefig(f'{CKPT_DIR}/training.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6Ô∏è‚É£ Export"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load best and export\n",
                "ckpt = torch.load(f'{CKPT_DIR}/best_detection.pt')\n",
                "model.load_state_dict(ckpt['model_state_dict'])\n",
                "model.eval()\n",
                "model.fuse_for_inference()\n",
                "model.cpu()\n",
                "\n",
                "# Export ONNX\n",
                "torch.onnx.export(\n",
                "    model, torch.randn(1, 3, 416, 416),\n",
                "    f'{CKPT_DIR}/detection.onnx',\n",
                "    input_names=['image'],\n",
                "    opset_version=12\n",
                ")\n",
                "\n",
                "print(f\"‚úÖ Exported to {CKPT_DIR}/detection.onnx\")\n",
                "print(f\"   Best loss: {ckpt['best_loss']:.4f}\")\n",
                "print(f\"\\nüì• Download from: {CKPT_DIR}/best_detection.pt\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}